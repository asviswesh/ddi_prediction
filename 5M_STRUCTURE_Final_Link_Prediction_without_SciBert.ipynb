{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPI9AO2sLSXy"
      },
      "source": [
        "# Graph Neural Networks for Link Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaJzlJO2Lcj8"
      },
      "source": [
        "Refer to this [blog post](https://medium.com/@tanishjain/224w-final-project-46c1054f2aa4) for more details!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wipe conflicting installs\n",
        "!pip -q uninstall -y torch torchvision torchaudio torch_geometric torch_scatter torch_sparse torch_cluster torch_spline_conv\n",
        "\n",
        "# Install PyTorch built for CUDA 12.1 (fits Colab GPU)\n",
        "!pip install -q --index-url https://download.pytorch.org/whl/cu121 \\\n",
        "  torch==2.4.0+cu121 torchvision==0.19.0+cu121 torchaudio==2.4.0+cu121\n",
        "\n",
        "# Install PyG and its compiled extensions matching that exact Torch/CUDA\n",
        "!pip install -q torch_geometric==2.5.3 \\\n",
        "  -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "\n",
        "# Optional packages\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "!pip install -q PyDrive\n",
        "!pip install ogb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDfu8UjWsKPu",
        "outputId": "6ed03ac3-68db-4294-f91a-94c932072515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch_geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch_scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch_sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch_cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch_spline_conv as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.1/799.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m186.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.5.0)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "OGB-ddi Link Prediction with Enh4SAGEConv (Torch 2.6 compatible)\n",
        "================================================================\n",
        "\n",
        "This single-file script integrates the fixes & improvements we discussed:\n",
        "\n",
        "Core fixes (most impactful):\n",
        "  1) NEGATIVE SAMPLING WITHOUT LEAKAGE:\n",
        "     - Build two graphs:\n",
        "         - edge_index_train : train-only (undirected, deduped) for message passing\n",
        "         - edge_index_all   : train ∪ valid ∪ test (undirected, deduped) for negative sampling\n",
        "     - This avoids sampling true edges (from valid/test) as negatives.\n",
        "\n",
        "  2) EVALUATION ROBUST TO GROUPED NEGATIVES:\n",
        "     - Supports both flat and grouped shapes of `edge_neg` in OGB.\n",
        "\n",
        "Training tweaks:\n",
        "  - Cosine decay (no warm restarts) with warmup\n",
        "  - Slightly lower dropout & edge_drop for DDI\n",
        "  - Fewer negatives per positive (neg_ratio=2)\n",
        "  - Freeze node embeddings for the first 10 epochs\n",
        "  - Early stopping by Val@20 (patience=40)\n",
        "  - AMP + grad clipping + AdamW remain\n",
        "  - Proximal regularizer to initial embeddings preserved\n",
        "\n",
        "Preserves:\n",
        "  - Google Drive artifact/result paths & file names\n",
        "  - 512-dim external node embeddings\n",
        "  - Enh4SAGEConv + Enh4SAGEStack w/ residuals, LayerNorm, DropEdge, JK-Max\n",
        "  - Plotting training loss and Hits@20 curves\n",
        "\n",
        "Tested with: Torch 2.6.0+cu124, PyG 2.6.1, ogb 1.3.x in Colab (Aug 2025).\n",
        "\n",
        "Changes made since running out of memory\n",
        "1) Set PYTORCH_CUDA_ALLOC_CONF at the very top;\n",
        "2) Call before eval:\n",
        "   if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "# os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:256\")\n",
        "\n",
        "import sys, math, random, json\n",
        "from pathlib import Path\n",
        "\n",
        "# ----------------------------- Minimal deps ----------------------------------\n",
        "def ensure_pkg(pkg: str, pip_name: str = None):\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "    except Exception:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name or pkg])\n",
        "\n",
        "# We do not pin torch/pyg here; your Colab already has 2.6.0/2.6.1.\n",
        "ensure_pkg(\"ogb\")\n",
        "ensure_pkg(\"networkx\", \"networkx>=3.0\")\n",
        "ensure_pkg(\"pydrive\", \"PyDrive\")\n",
        "\n",
        "import torch\n",
        "# ---- Torch 2.6 fix: default weights_only=True breaks OGB/PyG processed files.\n",
        "#      Force weights_only=False for ALL torch.load calls unless explicitly set.\n",
        "if \"weights_only\" in torch.load.__code__.co_varnames:\n",
        "    _orig_load = torch.load\n",
        "    def _load_compat(*args, **kwargs):\n",
        "        kwargs.setdefault(\"weights_only\", False)\n",
        "        return _orig_load(*args, **kwargs)\n",
        "    torch.load = _load_compat  # monkey-patch early\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import networkx as nx\n",
        "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
        "\n",
        "import torch_geometric as pyg\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import negative_sampling, to_networkx\n",
        "from torch_geometric.data import Data\n",
        "from torch.utils.data import DataLoader  # safer generic loader\n",
        "\n",
        "# ------------------------------ Colab Drive ----------------------------------\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ------------------------------ Paths ----------------------------------------\n",
        "BASE_DIR = \"/content/drive/MyDrive/CS145/Week11-GCN_SciBert_Ini\"\n",
        "ART_DIR  = f\"{BASE_DIR}/artifacts\"\n",
        "RES_DIR  = f\"{BASE_DIR}/results\"\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "os.makedirs(RES_DIR, exist_ok=True)\n",
        "\n",
        "EMB_PATH = f\"{ART_DIR}/projected_embeddings_512.pt\"  # [N,512]\n",
        "SPD_PATH = f\"{ART_DIR}/shortest_paths.pt\"\n",
        "EA_PATH  = f\"{ART_DIR}/edge_attr.pt\"\n",
        "\n",
        "# ------------------------------ Device/seed ----------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"CPU\")\n",
        "print(\"PyG:\", pyg.__version__)\n",
        "\n",
        "def set_all_seeds(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_all_seeds(42)\n",
        "os.environ.setdefault(\"PYTORCH_DISABLE_DYNAMO\", \"1\")\n",
        "os.environ.setdefault(\"TORCH_COMPILE_DISABLE\", \"1\")\n",
        "\n",
        "# ------------------------------ Dataset --------------------------------------\n",
        "dataset = PygLinkPropPredDataset(name=\"ogbl-ddi\", root='./dataset/')\n",
        "data_obj = dataset[0]\n",
        "split_edge = dataset.get_edge_split()\n",
        "evaluator = Evaluator(name='ogbl-ddi')\n",
        "\n",
        "num_nodes = data_obj.num_nodes\n",
        "edge_index_raw = data_obj.edge_index.to(device)\n",
        "print(f\"Loaded raw graph: {num_nodes} nodes, {edge_index_raw.size(1)} edges\")\n",
        "\n",
        "# ---------------------------- Build Train/All graphs -------------------------\n",
        "def to_undirected_coalesce(ei: torch.Tensor) -> torch.Tensor:\n",
        "    # Make undirected and dedupe edges.\n",
        "    ei_ud = torch.cat([ei, ei.flip(0)], dim=1)\n",
        "    ei_ud = torch.unique(ei_ud.t(), dim=0).t().contiguous()\n",
        "    return ei_ud\n",
        "\n",
        "train_ei = split_edge['train']['edge'].to(device).t().contiguous()\n",
        "valid_ei = split_edge['valid']['edge'].to(device).t().contiguous()\n",
        "test_ei  = split_edge['test']['edge'].to(device).t().contiguous()\n",
        "\n",
        "edge_index_train = to_undirected_coalesce(train_ei)\n",
        "edge_index_all   = to_undirected_coalesce(torch.cat([train_ei, valid_ei, test_ei], dim=1))\n",
        "\n",
        "print(f\"edge_index_train: {edge_index_train.size(1)} undirected edges\")\n",
        "print(f\"edge_index_all  : {edge_index_all.size(1)} undirected edges (for neg sampling only)\")\n",
        "\n",
        "# ---------------------- Shortest paths & edge attributes ---------------------\n",
        "def get_spd_matrix(G: nx.Graph, anchors, max_spd=5):\n",
        "    spd = np.zeros((G.number_of_nodes(), len(anchors)), dtype=np.float32)\n",
        "    for i, a in enumerate(anchors):\n",
        "        for node, L in nx.shortest_path_length(G, source=int(a)).items():\n",
        "            spd[int(node), i] = min(L, max_spd)\n",
        "    return spd\n",
        "\n",
        "def compute_anchor_distances(num_nodes, edge_index, num_anchors=500, max_path_length=5, device='cpu'):\n",
        "    \"\"\"\n",
        "    Compute SPD to random anchors on the TRAIN graph (avoid leakage).\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    # Build a minimal PyG Data for the train graph\n",
        "    d = Data(num_nodes=num_nodes, edge_index=edge_index)\n",
        "    G = to_networkx(d, to_undirected=True)\n",
        "    anchors = np.random.choice(G.number_of_nodes(), size=min(num_anchors, num_nodes), replace=False)\n",
        "    spd = get_spd_matrix(G, anchors, max_spd=max_path_length)\n",
        "    return torch.tensor(spd, dtype=torch.float32, device=device)  # [N, A]\n",
        "\n",
        "def prepare_edge_attributes(shortest_paths_to_anchors, edge_index, num_samples=5):\n",
        "    \"\"\"\n",
        "    Matches prior construction, but built on TRAIN edges:\n",
        "      - SPD rows for endpoints -> mean -> [E, A]\n",
        "      - For each of S samples, pick 200 anchors, mean -> [E, S]\n",
        "      - Per-column min-max normalize to [0,1]\n",
        "    \"\"\"\n",
        "    E = edge_index.size(1)\n",
        "    N, A = shortest_paths_to_anchors.shape\n",
        "\n",
        "    spa = shortest_paths_to_anchors[edge_index, :].mean(dim=0)  # [E, A]\n",
        "\n",
        "    rng = np.random.default_rng(42)\n",
        "    pick = min(200, A)\n",
        "    masks = np.stack([rng.choice(A, size=pick, replace=False) for _ in range(num_samples)], axis=0)  # [S, pick]\n",
        "    masks_t = torch.tensor(masks, device=spa.device, dtype=torch.long)\n",
        "\n",
        "    ea = spa[:, masks_t].mean(dim=2)  # [E, S]\n",
        "    a_max = ea.max(dim=0, keepdim=True).values\n",
        "    a_min = ea.min(dim=0, keepdim=True).values\n",
        "    ea = (ea - a_min) / (a_max - a_min + 1e-6)\n",
        "    return ea\n",
        "\n",
        "# Always recompute on TRAIN graph to avoid leakage and overwrite previous files.\n",
        "print(\"Computing shortest_paths and edge_attr on TRAIN graph (overwriting any existing files to avoid leakage)...\")\n",
        "shortest_paths = compute_anchor_distances(num_nodes, edge_index_train, num_anchors=500, max_path_length=5, device=device)\n",
        "edge_attr_full = prepare_edge_attributes(shortest_paths, edge_index_train, num_samples=5)\n",
        "torch.save(shortest_paths, SPD_PATH)\n",
        "torch.save(edge_attr_full, EA_PATH)\n",
        "print(f\"Saved shortest_paths -> {SPD_PATH}\")\n",
        "print(f\"Saved edge_attr     -> {EA_PATH}\")\n",
        "print(\"edge_attr shape:\", tuple(edge_attr_full.shape))  # [E_train, S]\n",
        "assert edge_attr_full.dim() == 2\n",
        "\n",
        "# ---------------------------- 512-dim embeddings -----------------------------\n",
        "Z = torch.load(EMB_PATH, map_location=device).float()\n",
        "assert Z.ndim == 2 and Z.shape[1] == 512 and Z.shape[0] == num_nodes, f\"Expected [{num_nodes},512], got {Z.shape}\"\n",
        "\n",
        "emb = torch.nn.Embedding(num_nodes, 512).to(device)\n",
        "#emb = nn.Embedding.from_pretrained(Z, freeze=False).to(device)\n",
        "E0  = nn.Embedding(num_nodes, Z.size(1)).to(device)\n",
        "E0.weight.data.copy_(emb.weight.data)   # snapshot of initial features\n",
        "\n",
        "# ----------------------------- Enh4SAGEConv ----------------------------------\n",
        "from typing import Union, Tuple\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
        "\n",
        "class Enh4SAGEConv(MessagePassing):\n",
        "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
        "                 out_channels: int, edge_attr_dim: int, normalize: bool = False,\n",
        "                 root_weight: bool = True, bias: bool = True, **kwargs):\n",
        "        kwargs.setdefault('aggr', 'mean')\n",
        "        super().__init__(**kwargs)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.normalize = normalize\n",
        "        self.root_weight = root_weight\n",
        "        self.edge_attr_dim = edge_attr_dim\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
        "        if self.root_weight:\n",
        "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
        "\n",
        "        self.lin_edge = Linear(edge_attr_dim, in_channels[0], bias=False)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin_l.reset_parameters()\n",
        "        if self.root_weight:\n",
        "            self.lin_r.reset_parameters()\n",
        "        self.lin_edge.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
        "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
        "        if isinstance(x, Tensor):\n",
        "            x = (x, x)\n",
        "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
        "        out = self.lin_l(out)\n",
        "        if self.root_weight and x[1] is not None:\n",
        "            out = out + self.lin_r(x[1])\n",
        "        if self.normalize:\n",
        "            out = F.normalize(out, p=2., dim=-1)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
        "        embedded_edge_attr = self.lin_edge(edge_attr)\n",
        "        return F.relu(x_j + embedded_edge_attr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels}, {self.out_channels})'\n",
        "\n",
        "# -------------------------- Enhanced SAGE Stack ------------------------------\n",
        "class Enh4SAGEStack(nn.Module):\n",
        "    \"\"\"\n",
        "    Residuals + LayerNorm + DropEdge + JK-Max around Enh4SAGEConv.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
        "                 num_layers, dropout, edge_attr_dim, edge_drop=0.05, jk=\"max\"):\n",
        "        super().__init__()\n",
        "        assert num_layers >= 2\n",
        "        self.edge_drop = edge_drop\n",
        "        self.dropout   = dropout\n",
        "        self.jk        = jk\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.norms = nn.ModuleList()\n",
        "        self.convs.append(Enh4SAGEConv(in_channels, hidden_channels, edge_attr_dim))\n",
        "        self.norms.append(nn.LayerNorm(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(Enh4SAGEConv(hidden_channels, hidden_channels, edge_attr_dim))\n",
        "            self.norms.append(nn.LayerNorm(hidden_channels))\n",
        "        self.convs.append(Enh4SAGEConv(hidden_channels, hidden_channels, edge_attr_dim))\n",
        "        self.norms.append(nn.LayerNorm(hidden_channels))\n",
        "\n",
        "        self.proj = nn.Identity() if hidden_channels == out_channels else nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for c in self.convs:\n",
        "            c.reset_parameters()\n",
        "        for n in self.norms:\n",
        "            if hasattr(n, \"reset_parameters\"):\n",
        "                n.reset_parameters()\n",
        "        if isinstance(self.proj, nn.Linear):\n",
        "            nn.init.xavier_uniform_(self.proj.weight); nn.init.zeros_(self.proj.bias)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, *_):\n",
        "        xs = []\n",
        "        E = edge_index.size(1)\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            if self.training and self.edge_drop > 0:\n",
        "                mask = torch.rand(E, device=edge_index.device) > self.edge_drop\n",
        "                ei = edge_index[:, mask]\n",
        "                ea = edge_attr[mask]\n",
        "            else:\n",
        "                ei, ea = edge_index, edge_attr\n",
        "            h = conv(x, ei, ea)\n",
        "            h = self.norms[i](h)\n",
        "            h = F.relu(h)\n",
        "            if h.shape == x.shape:\n",
        "                h = h + x\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            x = h\n",
        "            xs.append(h)\n",
        "        if self.jk == \"max\":\n",
        "            h_out = torch.stack(xs, dim=0).max(dim=0).values\n",
        "        else:\n",
        "            h_out = xs[-1]\n",
        "        return self.proj(h_out)\n",
        "\n",
        "# ------------------------------ Predictor ------------------------------------\n",
        "class LinkPredictor(nn.Module):\n",
        "    \"\"\"Returns logits.\"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels=512, num_layers=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        f_in = 4 * in_channels  # [hi*hj, |hi-hj|, hi, hj]\n",
        "        layers = []\n",
        "        dim = f_in\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers += [nn.Linear(dim, hidden_channels), nn.ReLU(), nn.Dropout(dropout)]\n",
        "            dim = hidden_channels\n",
        "        layers += [nn.Linear(dim, 1)]\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, hi, hj):\n",
        "        x = torch.cat([hi * hj, torch.abs(hi - hj), hi, hj], dim=-1)\n",
        "        return self.mlp(x).view(-1)  # logits\n",
        "\n",
        "# ------------------------- Eval (Hits@K via OGB) -----------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, predictor, edge_index_msg, edge_attr_msg, x, batch_size, split_edge, evaluator):\n",
        "    model.eval(); predictor.eval()\n",
        "\n",
        "    use_amp = (x.is_cuda)  # or reuse H[\"use_amp\"]\n",
        "    with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "      h = model(x, edge_index_msg, edge_attr_msg, None)\n",
        "\n",
        "    def score_pairs(edge_pairs):\n",
        "        out = []\n",
        "        for perm in DataLoader(range(edge_pairs.size(0)), batch_size=batch_size, shuffle=False):\n",
        "            e = edge_pairs[perm].t()\n",
        "            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "              out.append(torch.sigmoid(predictor(h[e[0]], h[e[1]])).cpu())\n",
        "        return torch.cat(out, dim=0)  # 1D\n",
        "\n",
        "    def score_maybe_grouped(neg):\n",
        "        # neg: either [M,2] or [G, R, 2]\n",
        "        if neg.dim() == 2:\n",
        "            return score_pairs(neg)                         # [M]\n",
        "        else:\n",
        "            G, R, _ = neg.shape\n",
        "            neg = neg.view(G*R, 2)\n",
        "            s = score_pairs(neg).view(G, R)                # [G, R]\n",
        "            return s\n",
        "\n",
        "    pos_valid = split_edge['valid']['edge'].to(h.device)\n",
        "    pos_test  = split_edge['test']['edge'].to(h.device)\n",
        "    neg_valid = split_edge['valid']['edge_neg'].to(h.device)\n",
        "    neg_test  = split_edge['test']['edge_neg'].to(h.device)\n",
        "\n",
        "    pos_valid_pred = score_pairs(pos_valid)\n",
        "    pos_test_pred  = score_pairs(pos_test)\n",
        "    neg_valid_pred = score_maybe_grouped(neg_valid)\n",
        "    neg_test_pred  = score_maybe_grouped(neg_test)\n",
        "\n",
        "    results = {}\n",
        "    for K in [20, 50, 100]:\n",
        "        evaluator.K = K\n",
        "        v = evaluator.eval({'y_pred_pos': pos_valid_pred, 'y_pred_neg': neg_valid_pred})[f'hits@{K}']\n",
        "        t = evaluator.eval({'y_pred_pos': pos_test_pred,  'y_pred_neg': neg_test_pred})[f'hits@{K}']\n",
        "        results[f'Hits@{K}'] = (v, t)\n",
        "    return results\n",
        "\n",
        "# --------------------------- Training primitives -----------------------------\n",
        "def bpr_loss(pos_logits: torch.Tensor, neg_logits: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Pairwise ranking loss. We align negatives into shape [B, R] (truncate/pad if needed).\n",
        "    \"\"\"\n",
        "    B = pos_logits.numel()\n",
        "    N = neg_logits.numel()\n",
        "    if B == 0 or N == 0:\n",
        "        return torch.tensor(0.0, device=pos_logits.device)\n",
        "\n",
        "    R = max(1, N // B)\n",
        "    needed = B * R\n",
        "\n",
        "    if N >= needed:\n",
        "        neg_use = neg_logits[:needed]\n",
        "    else:\n",
        "        reps = needed - N\n",
        "        pad = neg_logits[-1:].repeat(reps)\n",
        "        neg_use = torch.cat([neg_logits, pad], dim=0)\n",
        "\n",
        "    neg_mat = neg_use.view(B, R)                 # [B, R]\n",
        "    pos_mat = pos_logits.view(B, 1)              # [B, 1]\n",
        "    return F.softplus(-(pos_mat - neg_mat)).mean()\n",
        "\n",
        "def maybe_negative_sampling(edge_index, num_nodes, num_neg):\n",
        "    # Prefer 'sparse'; fallback to 'dense' if needed for compatibility.\n",
        "    try:\n",
        "        return negative_sampling(edge_index=edge_index, num_nodes=num_nodes,\n",
        "                                 num_neg_samples=num_neg, method='sparse')\n",
        "    except Exception:\n",
        "        return negative_sampling(edge_index=edge_index, num_nodes=num_nodes,\n",
        "                                 num_neg_samples=num_neg, method='dense')\n",
        "\n",
        "def train_one_epoch(model, predictor, x, E0, edge_index_msg, edge_attr_msg, edge_index_for_negs,\n",
        "                    pos_train_edges, optimizer, scaler, batch_size,\n",
        "                    neg_ratio=2, lam=5e-4, hard_frac=0.5, hard_mul=1.5, use_amp=True):\n",
        "    model.train(); predictor.train()\n",
        "    total_loss, total_examples = 0.0, 0\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    n = pos_train_edges.size(0)\n",
        "    for perm in DataLoader(range(n), batch_size=batch_size, shuffle=True):\n",
        "        idx = torch.as_tensor(perm, dtype=torch.long, device=device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(use_amp and device.type=='cuda')):\n",
        "            h = model(x, edge_index_msg, edge_attr_msg, None)\n",
        "\n",
        "            pos_edge = pos_train_edges[idx].t()     # [2, B]\n",
        "            pos_logits = predictor(h[pos_edge[0]], h[pos_edge[1]])  # [B]\n",
        "\n",
        "            neg_samples = maybe_negative_sampling(\n",
        "                edge_index=edge_index_for_negs,     # <-- ALL positives, to avoid false negatives\n",
        "                num_nodes=x.size(0),\n",
        "                num_neg=pos_logits.numel() * neg_ratio\n",
        "            )\n",
        "            neg_logits = predictor(h[neg_samples[0]], h[neg_samples[1]])  # [B*R]\n",
        "\n",
        "            # Hard-negative mixing\n",
        "            if hard_frac > 0.0 and neg_logits.numel() > 0:\n",
        "                k = max(1, int(hard_frac * neg_logits.numel()))\n",
        "                hard_vals, _ = torch.topk(neg_logits, k=k, largest=True, sorted=False)\n",
        "                extra = hard_vals.repeat_interleave(int(math.ceil(hard_mul)))\n",
        "                neg_logits_eff = torch.cat([neg_logits, extra], dim=0)\n",
        "            else:\n",
        "                neg_logits_eff = neg_logits\n",
        "\n",
        "            loss_rank = bpr_loss(pos_logits, neg_logits_eff)\n",
        "\n",
        "            labels = torch.cat([torch.ones_like(pos_logits), torch.zeros_like(neg_logits)], dim=0)\n",
        "            logits = torch.cat([pos_logits, neg_logits], dim=0)\n",
        "            loss_bce = bce(logits, labels)\n",
        "\n",
        "            touched = torch.unique(torch.cat([pos_edge.reshape(-1), neg_samples.reshape(-1)], dim=0))\n",
        "            prior = (E0(touched) if hasattr(E0, \"__call__\") else E0[touched]).detach()\n",
        "            prox = lam * (x[touched] - prior).pow(2).mean()\n",
        "\n",
        "            # loss = loss_rank + 0.15 * loss_bce + prox\n",
        "            loss = loss_rank + 0.15 * loss_bce\n",
        "\n",
        "        if scaler is not None and device.type == \"cuda\" and use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * pos_logits.size(0)\n",
        "        total_examples += pos_logits.size(0)\n",
        "\n",
        "    return total_loss / max(1, total_examples)\n",
        "\n",
        "# ------------------------------- Hyperparams ---------------------------------\n",
        "H = {\n",
        "    \"epochs\":          400,\n",
        "    \"hidden_dim\":      512,\n",
        "    \"dropout\":         0.2,      # slightly lower for DDI\n",
        "    \"num_layers\":      3,\n",
        "    \"lr_main\":         1e-3,\n",
        "    \"lr_emb\":          5e-4,\n",
        "    \"weight_decay\":    0.01,\n",
        "    \"batch_size\":      64 * 1024,\n",
        "    \"neg_ratio\":       2,        # fewer, cleaner negatives\n",
        "    \"lam_prox\":        5e-4,\n",
        "    \"edge_drop\":       0.05,     # gentler drop edge\n",
        "    \"use_amp\":         True,\n",
        "    \"warmup_epochs\":   10,\n",
        "    \"patience\":        40,       # early stopping by Val@20\n",
        "}\n",
        "\n",
        "# ----------------------------- Build + Optimizer -----------------------------\n",
        "edge_attr_full = edge_attr_full.to(device)\n",
        "\n",
        "model = Enh4SAGEStack(\n",
        "    in_channels=emb.embedding_dim,\n",
        "    hidden_channels=H[\"hidden_dim\"],\n",
        "    out_channels=H[\"hidden_dim\"],\n",
        "    num_layers=H[\"num_layers\"],\n",
        "    dropout=H[\"dropout\"],\n",
        "    edge_attr_dim=edge_attr_full.size(1),\n",
        "    edge_drop=H[\"edge_drop\"],\n",
        "    jk=\"max\"\n",
        ").to(device)\n",
        "\n",
        "predictor = LinkPredictor(\n",
        "    in_channels=H[\"hidden_dim\"],\n",
        "    hidden_channels=H[\"hidden_dim\"],\n",
        "    num_layers=3,\n",
        "    dropout=H[\"dropout\"]\n",
        ").to(device)\n",
        "\n",
        "def count_params(m): return sum(p.numel() for p in m.parameters())\n",
        "print(\"Parameters:\")\n",
        "print(\"  GNN       :\", count_params(model))\n",
        "print(\"  Predictor :\", count_params(predictor))\n",
        "print(\"  Embedding :\", count_params(emb))\n",
        "\n",
        "param_groups = [\n",
        "    {\"params\": model.parameters()},\n",
        "    {\"params\": predictor.parameters()},\n",
        "    {\"params\": emb.parameters(), \"lr\": H[\"lr_emb\"]},\n",
        "]\n",
        "optimizer = torch.optim.AdamW(param_groups, lr=H[\"lr_main\"], weight_decay=H[\"weight_decay\"])\n",
        "\n",
        "def lr_lambda_warmup(epoch):\n",
        "    if epoch < H[\"warmup_epochs\"]:\n",
        "        return float(epoch + 1) / float(max(1, H[\"warmup_epochs\"]))\n",
        "    return 1.0\n",
        "\n",
        "warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_warmup)\n",
        "# Cosine decay (no warm restarts)\n",
        "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=max(1, H[\"epochs\"] - H[\"warmup_epochs\"])\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\" and H[\"use_amp\"]))\n",
        "\n",
        "# --------------------------------- Train ------------------------------------\n",
        "pos_train_edges = split_edge['train']['edge'].to(device)\n",
        "\n",
        "best = {\"val\": 0.0, \"test\": 0.0, \"epoch\": -1}\n",
        "train_loss_hist, val_hist, test_hist = [], [], []\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Optional: quick sanity check that false-negative rate is ~0 with ALL graph\n",
        "with torch.no_grad():\n",
        "    # Sample a bunch of negatives against ALL positives (should avoid true edges)\n",
        "    neg_tmp = maybe_negative_sampling(edge_index_all, num_nodes, 200000).t()\n",
        "    # Build a set of ALL positives for membership check\n",
        "    all_pos = torch.unique(edge_index_all.t(), dim=0)\n",
        "    # Check overlap in both directions (undirected)\n",
        "    pos_set = { (int(a), int(b)) for a,b in all_pos.tolist() }\n",
        "    pos_set |= { (b, a) for a,b in all_pos.tolist() }\n",
        "    leak = sum((int(a), int(b)) in pos_set for a,b in neg_tmp.tolist()) / max(1, neg_tmp.size(0))\n",
        "    print(f\"[Sanity] Approx false-negative rate vs ALL positives: {leak:.6f}\")\n",
        "\n",
        "torch.nn.init.xavier_uniform_(emb.weight)\n",
        "\n",
        "# Freeze node embeddings for warmup (stabilize)\n",
        "emb.requires_grad_(False)\n",
        "\n",
        "for epoch in range(1, H[\"epochs\"] + 1):\n",
        "    x_feats = emb.weight  # trainable features (grad may be disabled initially)\n",
        "\n",
        "    loss = train_one_epoch(\n",
        "        model, predictor, x_feats, E0,\n",
        "        edge_index_msg=edge_index_train, edge_attr_msg=edge_attr_full,\n",
        "        edge_index_for_negs=edge_index_all,\n",
        "        pos_train_edges=pos_train_edges,\n",
        "        optimizer=optimizer, scaler=scaler,\n",
        "        batch_size=H[\"batch_size\"], neg_ratio=H[\"neg_ratio\"], lam=H[\"lam_prox\"],\n",
        "        hard_frac=0.5, hard_mul=1.5, use_amp=H[\"use_amp\"]\n",
        "    )\n",
        "    train_loss_hist.append(loss)\n",
        "\n",
        "    # Unfreeze embeddings after warmup phase\n",
        "    if epoch == H[\"warmup_epochs\"] + 1:\n",
        "        emb.requires_grad_(True)\n",
        "\n",
        "    if epoch <= H[\"warmup_epochs\"]:\n",
        "        warmup.step()\n",
        "    else:\n",
        "        cosine.step()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    results = evaluate(\n",
        "        model, predictor,\n",
        "        edge_index_msg=edge_index_train, edge_attr_msg=edge_attr_full, x=x_feats,\n",
        "        batch_size=H[\"batch_size\"], split_edge=split_edge, evaluator=evaluator\n",
        "    )\n",
        "    val20, test20 = results[\"Hits@20\"]\n",
        "    val_hist.append(val20); test_hist.append(test20)\n",
        "\n",
        "    lr_str = \", \".join([f\"{pg['lr']:.6f}\" for pg in optimizer.param_groups[:2]])\n",
        "    print(f\"Epoch {epoch:03d} | loss {loss:.5f} | lr [{lr_str}] | Val@20 {val20:.4f}  Test@20 {test20:.4f}\")\n",
        "\n",
        "    improved = val20 > best[\"val\"]\n",
        "    if improved:\n",
        "        best.update({\"val\": val20, \"test\": test20, \"epoch\": epoch})\n",
        "        # Save best checkpoints\n",
        "        torch.save(model.state_dict(), f\"{ART_DIR}/best_model.pt\")\n",
        "        torch.save(predictor.state_dict(), f\"{ART_DIR}/best_predictor.pt\")\n",
        "        torch.save(emb.state_dict(), f\"{ART_DIR}/best_emb.pt\")\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= H[\"patience\"]:\n",
        "        print(f\"Early stopping at epoch {epoch} (no improvement for {H['patience']} epochs).\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nBest: epoch {best['epoch']} | Val@20 {best['val']:.4f} | Test@20 {best['test']:.4f}\")\n",
        "\n",
        "# --------------------------------- Plot -------------------------------------\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Link Prediction on OGB-ddi with Enh4SAGEConv (Torch 2.6 fixed)')\n",
        "plt.plot(train_loss_hist, label='train loss')\n",
        "plt.plot(val_hist, label='Hits@20 val')\n",
        "plt.plot(test_hist, label='Hits@20 test')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Metric')\n",
        "plt.grid(True); plt.legend()\n",
        "plot_path = f\"{RES_DIR}/{num_nodes}_Enh4Sage_T26_plot.png\"\n",
        "plt.savefig(plot_path); plt.close()\n",
        "print(f\"Plot saved to {plot_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx5cVLM5ctA2",
        "outputId": "696c5454-426a-49c7-fbdb-85e8b2f2f28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cuda\n",
            "Torch: 2.4.0+cu121 CUDA: 12.1\n",
            "PyG: 2.5.3\n",
            "Downloading http://snap.stanford.edu/ogb/data/linkproppred/ddi.zip\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 0.04 GB: 100%|██████████| 46/46 [00:07<00:00,  6.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./dataset/ddi.zip\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 51.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1411.75it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded raw graph: 4267 nodes, 2135822 edges\n",
            "edge_index_train: 2135822 undirected edges\n",
            "edge_index_all  : 2669778 undirected edges (for neg sampling only)\n",
            "Computing shortest_paths and edge_attr on TRAIN graph (overwriting any existing files to avoid leakage)...\n",
            "Saved shortest_paths -> /content/drive/MyDrive/CS145/Week11-GCN_SciBert_Ini/artifacts/shortest_paths.pt\n",
            "Saved edge_attr     -> /content/drive/MyDrive/CS145/Week11-GCN_SciBert_Ini/artifacts/edge_attr.pt\n",
            "edge_attr shape: (2135822, 5)\n",
            "Parameters:\n",
            "  GNN       : 1585152\n",
            "  Predictor : 1312257\n",
            "  Embedding : 2184704\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2362693254.py:548: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\" and H[\"use_amp\"]))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sanity] Approx false-negative rate vs ALL positives: 0.000000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2362693254.py:433: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(use_amp and device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | loss 0.69905 | lr [0.000200, 0.000200] | Val@20 0.0262  Test@20 0.0582\n",
            "Epoch 002 | loss 0.47865 | lr [0.000300, 0.000300] | Val@20 0.0179  Test@20 0.0326\n",
            "Epoch 003 | loss 0.38939 | lr [0.000400, 0.000400] | Val@20 0.0500  Test@20 0.0100\n",
            "Epoch 004 | loss 0.34259 | lr [0.000500, 0.000500] | Val@20 0.0426  Test@20 0.0073\n",
            "Epoch 005 | loss 0.29168 | lr [0.000600, 0.000600] | Val@20 0.1160  Test@20 0.0657\n",
            "Epoch 006 | loss 0.27287 | lr [0.000700, 0.000700] | Val@20 0.1320  Test@20 0.0828\n",
            "Epoch 007 | loss 0.25300 | lr [0.000800, 0.000800] | Val@20 0.1459  Test@20 0.1064\n",
            "Epoch 008 | loss 0.21846 | lr [0.000900, 0.000900] | Val@20 0.1392  Test@20 0.1267\n",
            "Epoch 009 | loss 0.18070 | lr [0.001000, 0.001000] | Val@20 0.1728  Test@20 0.1302\n",
            "Epoch 010 | loss 0.15884 | lr [0.001000, 0.001000] | Val@20 0.1691  Test@20 0.1418\n",
            "Epoch 011 | loss 0.15321 | lr [0.001000, 0.001000] | Val@20 0.2597  Test@20 0.1771\n",
            "Epoch 012 | loss 0.11186 | lr [0.001000, 0.001000] | Val@20 0.3895  Test@20 0.2908\n",
            "Epoch 013 | loss 0.08158 | lr [0.001000, 0.001000] | Val@20 0.4217  Test@20 0.2788\n",
            "Epoch 014 | loss 0.06677 | lr [0.001000, 0.001000] | Val@20 0.4554  Test@20 0.3150\n",
            "Epoch 015 | loss 0.05888 | lr [0.001000, 0.001000] | Val@20 0.5218  Test@20 0.3702\n",
            "Epoch 016 | loss 0.05317 | lr [0.000999, 0.000999] | Val@20 0.5170  Test@20 0.3764\n",
            "Epoch 017 | loss 0.04780 | lr [0.000999, 0.000999] | Val@20 0.5267  Test@20 0.4089\n",
            "Epoch 018 | loss 0.04375 | lr [0.000999, 0.000999] | Val@20 0.5679  Test@20 0.4336\n",
            "Epoch 019 | loss 0.04054 | lr [0.000999, 0.000999] | Val@20 0.5835  Test@20 0.4508\n",
            "Epoch 020 | loss 0.03736 | lr [0.000998, 0.000998] | Val@20 0.6112  Test@20 0.4704\n",
            "Epoch 021 | loss 0.03474 | lr [0.000998, 0.000998] | Val@20 0.6270  Test@20 0.4799\n",
            "Epoch 022 | loss 0.03218 | lr [0.000998, 0.000998] | Val@20 0.6196  Test@20 0.4739\n",
            "Epoch 023 | loss 0.03007 | lr [0.000997, 0.000997] | Val@20 0.6336  Test@20 0.5031\n",
            "Epoch 024 | loss 0.02792 | lr [0.000997, 0.000997] | Val@20 0.6456  Test@20 0.5302\n",
            "Epoch 025 | loss 0.02561 | lr [0.000996, 0.000996] | Val@20 0.6522  Test@20 0.5481\n",
            "Epoch 026 | loss 0.02414 | lr [0.000996, 0.000996] | Val@20 0.6683  Test@20 0.5521\n",
            "Epoch 027 | loss 0.02306 | lr [0.000995, 0.000995] | Val@20 0.6594  Test@20 0.5396\n",
            "Epoch 028 | loss 0.02190 | lr [0.000995, 0.000995] | Val@20 0.6807  Test@20 0.5646\n",
            "Epoch 029 | loss 0.02075 | lr [0.000994, 0.000994] | Val@20 0.6860  Test@20 0.5748\n",
            "Epoch 030 | loss 0.01958 | lr [0.000994, 0.000994] | Val@20 0.7065  Test@20 0.5946\n",
            "Epoch 031 | loss 0.01840 | lr [0.000993, 0.000993] | Val@20 0.7122  Test@20 0.5892\n",
            "Epoch 032 | loss 0.01718 | lr [0.000992, 0.000992] | Val@20 0.7300  Test@20 0.6254\n",
            "Epoch 033 | loss 0.01660 | lr [0.000991, 0.000991] | Val@20 0.7312  Test@20 0.6155\n",
            "Epoch 034 | loss 0.01565 | lr [0.000991, 0.000991] | Val@20 0.7352  Test@20 0.6242\n",
            "Epoch 035 | loss 0.01474 | lr [0.000990, 0.000990] | Val@20 0.7472  Test@20 0.6311\n",
            "Epoch 036 | loss 0.01391 | lr [0.000989, 0.000989] | Val@20 0.7505  Test@20 0.6245\n",
            "Epoch 037 | loss 0.01343 | lr [0.000988, 0.000988] | Val@20 0.7795  Test@20 0.6572\n",
            "Epoch 038 | loss 0.01289 | lr [0.000987, 0.000987] | Val@20 0.7954  Test@20 0.6787\n",
            "Epoch 039 | loss 0.01209 | lr [0.000986, 0.000986] | Val@20 0.7868  Test@20 0.6787\n",
            "Epoch 040 | loss 0.01185 | lr [0.000985, 0.000985] | Val@20 0.8049  Test@20 0.6763\n",
            "Epoch 041 | loss 0.01133 | lr [0.000984, 0.000984] | Val@20 0.8111  Test@20 0.6997\n",
            "Epoch 042 | loss 0.01099 | lr [0.000983, 0.000983] | Val@20 0.8132  Test@20 0.6862\n",
            "Epoch 043 | loss 0.01050 | lr [0.000982, 0.000982] | Val@20 0.8184  Test@20 0.6975\n",
            "Epoch 044 | loss 0.01018 | lr [0.000981, 0.000981] | Val@20 0.8034  Test@20 0.7032\n",
            "Epoch 045 | loss 0.00974 | lr [0.000980, 0.000980] | Val@20 0.8178  Test@20 0.7255\n",
            "Epoch 046 | loss 0.00952 | lr [0.000979, 0.000979] | Val@20 0.8085  Test@20 0.6982\n",
            "Epoch 047 | loss 0.00924 | lr [0.000978, 0.000978] | Val@20 0.8233  Test@20 0.7179\n",
            "Epoch 048 | loss 0.00901 | lr [0.000977, 0.000977] | Val@20 0.8367  Test@20 0.7299\n",
            "Epoch 049 | loss 0.00886 | lr [0.000976, 0.000976] | Val@20 0.8377  Test@20 0.7158\n",
            "Epoch 050 | loss 0.00853 | lr [0.000974, 0.000974] | Val@20 0.8449  Test@20 0.7274\n",
            "Epoch 051 | loss 0.00826 | lr [0.000973, 0.000973] | Val@20 0.8422  Test@20 0.7229\n",
            "Epoch 052 | loss 0.00826 | lr [0.000972, 0.000972] | Val@20 0.8470  Test@20 0.7366\n",
            "Epoch 053 | loss 0.00778 | lr [0.000970, 0.000970] | Val@20 0.8423  Test@20 0.7416\n",
            "Epoch 054 | loss 0.00753 | lr [0.000969, 0.000969] | Val@20 0.8249  Test@20 0.7385\n",
            "Epoch 055 | loss 0.00740 | lr [0.000968, 0.000968] | Val@20 0.8500  Test@20 0.7517\n",
            "Epoch 056 | loss 0.00722 | lr [0.000966, 0.000966] | Val@20 0.8618  Test@20 0.7536\n",
            "Epoch 057 | loss 0.00704 | lr [0.000965, 0.000965] | Val@20 0.8641  Test@20 0.7536\n",
            "Epoch 058 | loss 0.00704 | lr [0.000963, 0.000963] | Val@20 0.8497  Test@20 0.7402\n",
            "Epoch 059 | loss 0.00677 | lr [0.000962, 0.000962] | Val@20 0.8739  Test@20 0.7784\n",
            "Epoch 060 | loss 0.00661 | lr [0.000960, 0.000960] | Val@20 0.8659  Test@20 0.7581\n",
            "Epoch 061 | loss 0.00646 | lr [0.000958, 0.000958] | Val@20 0.8621  Test@20 0.7856\n",
            "Epoch 062 | loss 0.00641 | lr [0.000957, 0.000957] | Val@20 0.8695  Test@20 0.7625\n",
            "Epoch 063 | loss 0.00627 | lr [0.000955, 0.000955] | Val@20 0.8844  Test@20 0.7922\n",
            "Epoch 064 | loss 0.00608 | lr [0.000953, 0.000953] | Val@20 0.8764  Test@20 0.7716\n",
            "Epoch 065 | loss 0.00603 | lr [0.000952, 0.000952] | Val@20 0.8793  Test@20 0.7875\n",
            "Epoch 066 | loss 0.00581 | lr [0.000950, 0.000950] | Val@20 0.8795  Test@20 0.7901\n",
            "Epoch 067 | loss 0.00579 | lr [0.000948, 0.000948] | Val@20 0.8797  Test@20 0.7887\n",
            "Epoch 068 | loss 0.00554 | lr [0.000946, 0.000946] | Val@20 0.8800  Test@20 0.7969\n",
            "Epoch 069 | loss 0.00548 | lr [0.000945, 0.000945] | Val@20 0.8771  Test@20 0.7946\n",
            "Epoch 070 | loss 0.00540 | lr [0.000943, 0.000943] | Val@20 0.8930  Test@20 0.8000\n",
            "Epoch 071 | loss 0.00536 | lr [0.000941, 0.000941] | Val@20 0.8996  Test@20 0.7991\n",
            "Epoch 072 | loss 0.00518 | lr [0.000939, 0.000939] | Val@20 0.9011  Test@20 0.8030\n",
            "Epoch 073 | loss 0.00504 | lr [0.000937, 0.000937] | Val@20 0.8995  Test@20 0.8040\n",
            "Epoch 074 | loss 0.00499 | lr [0.000935, 0.000935] | Val@20 0.8967  Test@20 0.8151\n",
            "Epoch 075 | loss 0.00488 | lr [0.000933, 0.000933] | Val@20 0.9057  Test@20 0.8110\n",
            "Epoch 076 | loss 0.00482 | lr [0.000931, 0.000931] | Val@20 0.9056  Test@20 0.8065\n",
            "Epoch 077 | loss 0.00491 | lr [0.000929, 0.000929] | Val@20 0.8968  Test@20 0.8116\n",
            "Epoch 078 | loss 0.00468 | lr [0.000927, 0.000927] | Val@20 0.9082  Test@20 0.8134\n",
            "Epoch 079 | loss 0.00461 | lr [0.000925, 0.000925] | Val@20 0.9148  Test@20 0.8203\n",
            "Epoch 080 | loss 0.00442 | lr [0.000923, 0.000923] | Val@20 0.9067  Test@20 0.8168\n",
            "Epoch 081 | loss 0.00436 | lr [0.000920, 0.000920] | Val@20 0.9035  Test@20 0.8124\n",
            "Epoch 082 | loss 0.00440 | lr [0.000918, 0.000918] | Val@20 0.9063  Test@20 0.8192\n",
            "Epoch 083 | loss 0.00433 | lr [0.000916, 0.000916] | Val@20 0.9123  Test@20 0.8294\n",
            "Epoch 084 | loss 0.00426 | lr [0.000914, 0.000914] | Val@20 0.9115  Test@20 0.8229\n",
            "Epoch 085 | loss 0.00426 | lr [0.000911, 0.000911] | Val@20 0.9095  Test@20 0.8238\n",
            "Epoch 086 | loss 0.00406 | lr [0.000909, 0.000909] | Val@20 0.9119  Test@20 0.8243\n",
            "Epoch 087 | loss 0.00409 | lr [0.000907, 0.000907] | Val@20 0.9140  Test@20 0.8251\n",
            "Epoch 088 | loss 0.00388 | lr [0.000905, 0.000905] | Val@20 0.9207  Test@20 0.8198\n",
            "Epoch 089 | loss 0.00385 | lr [0.000902, 0.000902] | Val@20 0.9186  Test@20 0.8226\n",
            "Epoch 090 | loss 0.00385 | lr [0.000900, 0.000900] | Val@20 0.9151  Test@20 0.8277\n",
            "Epoch 091 | loss 0.00380 | lr [0.000897, 0.000897] | Val@20 0.9177  Test@20 0.8248\n",
            "Epoch 092 | loss 0.00375 | lr [0.000895, 0.000895] | Val@20 0.9206  Test@20 0.8328\n",
            "Epoch 093 | loss 0.00370 | lr [0.000892, 0.000892] | Val@20 0.9216  Test@20 0.8281\n",
            "Epoch 094 | loss 0.00367 | lr [0.000890, 0.000890] | Val@20 0.9226  Test@20 0.8282\n",
            "Epoch 095 | loss 0.00363 | lr [0.000887, 0.000887] | Val@20 0.9261  Test@20 0.8324\n",
            "Epoch 096 | loss 0.00366 | lr [0.000885, 0.000885] | Val@20 0.9218  Test@20 0.8356\n",
            "Epoch 097 | loss 0.00356 | lr [0.000882, 0.000882] | Val@20 0.9246  Test@20 0.8475\n",
            "Epoch 098 | loss 0.00345 | lr [0.000880, 0.000880] | Val@20 0.9294  Test@20 0.8403\n",
            "Epoch 099 | loss 0.00342 | lr [0.000877, 0.000877] | Val@20 0.9336  Test@20 0.8404\n",
            "Epoch 100 | loss 0.00336 | lr [0.000874, 0.000874] | Val@20 0.9366  Test@20 0.8351\n",
            "Epoch 101 | loss 0.00335 | lr [0.000872, 0.000872] | Val@20 0.9383  Test@20 0.8330\n",
            "Epoch 102 | loss 0.00341 | lr [0.000869, 0.000869] | Val@20 0.9357  Test@20 0.8457\n",
            "Epoch 103 | loss 0.00319 | lr [0.000866, 0.000866] | Val@20 0.9270  Test@20 0.8477\n",
            "Epoch 104 | loss 0.00322 | lr [0.000863, 0.000863] | Val@20 0.9362  Test@20 0.8457\n",
            "Epoch 105 | loss 0.00310 | lr [0.000861, 0.000861] | Val@20 0.9429  Test@20 0.8473\n",
            "Epoch 106 | loss 0.00325 | lr [0.000858, 0.000858] | Val@20 0.9372  Test@20 0.8461\n",
            "Epoch 107 | loss 0.00309 | lr [0.000855, 0.000855] | Val@20 0.9367  Test@20 0.8455\n",
            "Epoch 108 | loss 0.00308 | lr [0.000852, 0.000852] | Val@20 0.9403  Test@20 0.8446\n",
            "Epoch 109 | loss 0.00304 | lr [0.000849, 0.000849] | Val@20 0.9384  Test@20 0.8554\n",
            "Epoch 110 | loss 0.00297 | lr [0.000846, 0.000846] | Val@20 0.9420  Test@20 0.8605\n",
            "Epoch 111 | loss 0.00291 | lr [0.000843, 0.000843] | Val@20 0.9434  Test@20 0.8565\n",
            "Epoch 112 | loss 0.00289 | lr [0.000841, 0.000841] | Val@20 0.9403  Test@20 0.8655\n",
            "Epoch 113 | loss 0.00286 | lr [0.000838, 0.000838] | Val@20 0.9454  Test@20 0.8496\n",
            "Epoch 114 | loss 0.00285 | lr [0.000835, 0.000835] | Val@20 0.9468  Test@20 0.8471\n",
            "Epoch 115 | loss 0.00283 | lr [0.000832, 0.000832] | Val@20 0.9478  Test@20 0.8580\n",
            "Epoch 116 | loss 0.00282 | lr [0.000829, 0.000829] | Val@20 0.9483  Test@20 0.8561\n",
            "Epoch 117 | loss 0.00276 | lr [0.000825, 0.000825] | Val@20 0.9388  Test@20 0.8463\n",
            "Epoch 118 | loss 0.00274 | lr [0.000822, 0.000822] | Val@20 0.9422  Test@20 0.8546\n",
            "Epoch 119 | loss 0.00267 | lr [0.000819, 0.000819] | Val@20 0.9465  Test@20 0.8479\n",
            "Epoch 120 | loss 0.00259 | lr [0.000816, 0.000816] | Val@20 0.9467  Test@20 0.8557\n",
            "Epoch 121 | loss 0.00258 | lr [0.000813, 0.000813] | Val@20 0.9445  Test@20 0.8518\n",
            "Epoch 122 | loss 0.00266 | lr [0.000810, 0.000810] | Val@20 0.9414  Test@20 0.8566\n",
            "Epoch 123 | loss 0.00266 | lr [0.000807, 0.000807] | Val@20 0.9513  Test@20 0.8535\n",
            "Epoch 124 | loss 0.00256 | lr [0.000804, 0.000804] | Val@20 0.9440  Test@20 0.8571\n",
            "Epoch 125 | loss 0.00255 | lr [0.000800, 0.000800] | Val@20 0.9513  Test@20 0.8600\n",
            "Epoch 126 | loss 0.00252 | lr [0.000797, 0.000797] | Val@20 0.9538  Test@20 0.8646\n",
            "Epoch 127 | loss 0.00249 | lr [0.000794, 0.000794] | Val@20 0.9483  Test@20 0.8638\n",
            "Epoch 128 | loss 0.00241 | lr [0.000791, 0.000791] | Val@20 0.9517  Test@20 0.8707\n",
            "Epoch 129 | loss 0.00250 | lr [0.000787, 0.000787] | Val@20 0.9501  Test@20 0.8761\n",
            "Epoch 130 | loss 0.00249 | lr [0.000784, 0.000784] | Val@20 0.9513  Test@20 0.8695\n",
            "Epoch 131 | loss 0.00240 | lr [0.000781, 0.000781] | Val@20 0.9506  Test@20 0.8619\n",
            "Epoch 132 | loss 0.00234 | lr [0.000777, 0.000777] | Val@20 0.9484  Test@20 0.8579\n",
            "Epoch 133 | loss 0.00235 | lr [0.000774, 0.000774] | Val@20 0.9513  Test@20 0.8649\n",
            "Epoch 134 | loss 0.00224 | lr [0.000771, 0.000771] | Val@20 0.9508  Test@20 0.8648\n",
            "Epoch 135 | loss 0.00222 | lr [0.000767, 0.000767] | Val@20 0.9521  Test@20 0.8600\n",
            "Epoch 136 | loss 0.00225 | lr [0.000764, 0.000764] | Val@20 0.9553  Test@20 0.8689\n",
            "Epoch 137 | loss 0.00218 | lr [0.000760, 0.000760] | Val@20 0.9537  Test@20 0.8631\n",
            "Epoch 138 | loss 0.00220 | lr [0.000757, 0.000757] | Val@20 0.9521  Test@20 0.8641\n",
            "Epoch 139 | loss 0.00224 | lr [0.000753, 0.000753] | Val@20 0.9566  Test@20 0.8737\n",
            "Epoch 140 | loss 0.00217 | lr [0.000750, 0.000750] | Val@20 0.9521  Test@20 0.8661\n",
            "Epoch 141 | loss 0.00215 | lr [0.000747, 0.000747] | Val@20 0.9520  Test@20 0.8691\n",
            "Epoch 142 | loss 0.00216 | lr [0.000743, 0.000743] | Val@20 0.9563  Test@20 0.8751\n",
            "Epoch 143 | loss 0.00213 | lr [0.000739, 0.000739] | Val@20 0.9543  Test@20 0.8785\n",
            "Epoch 144 | loss 0.00209 | lr [0.000736, 0.000736] | Val@20 0.9540  Test@20 0.8732\n",
            "Epoch 145 | loss 0.00206 | lr [0.000732, 0.000732] | Val@20 0.9571  Test@20 0.8683\n",
            "Epoch 146 | loss 0.00201 | lr [0.000729, 0.000729] | Val@20 0.9544  Test@20 0.8716\n",
            "Epoch 147 | loss 0.00205 | lr [0.000725, 0.000725] | Val@20 0.9574  Test@20 0.8736\n",
            "Epoch 148 | loss 0.00200 | lr [0.000722, 0.000722] | Val@20 0.9599  Test@20 0.8824\n",
            "Epoch 149 | loss 0.00205 | lr [0.000718, 0.000718] | Val@20 0.9583  Test@20 0.8735\n",
            "Epoch 150 | loss 0.00198 | lr [0.000714, 0.000714] | Val@20 0.9611  Test@20 0.8706\n",
            "Epoch 151 | loss 0.00199 | lr [0.000711, 0.000711] | Val@20 0.9591  Test@20 0.8790\n",
            "Epoch 152 | loss 0.00197 | lr [0.000707, 0.000707] | Val@20 0.9573  Test@20 0.8702\n",
            "Epoch 153 | loss 0.00193 | lr [0.000703, 0.000703] | Val@20 0.9587  Test@20 0.8819\n",
            "Epoch 154 | loss 0.00197 | lr [0.000700, 0.000700] | Val@20 0.9569  Test@20 0.8810\n",
            "Epoch 155 | loss 0.00191 | lr [0.000696, 0.000696] | Val@20 0.9619  Test@20 0.8770\n",
            "Epoch 156 | loss 0.00187 | lr [0.000692, 0.000692] | Val@20 0.9614  Test@20 0.8866\n",
            "Epoch 157 | loss 0.00184 | lr [0.000689, 0.000689] | Val@20 0.9576  Test@20 0.8866\n",
            "Epoch 158 | loss 0.00189 | lr [0.000685, 0.000685] | Val@20 0.9590  Test@20 0.8902\n",
            "Epoch 159 | loss 0.00182 | lr [0.000681, 0.000681] | Val@20 0.9644  Test@20 0.8822\n",
            "Epoch 160 | loss 0.00184 | lr [0.000677, 0.000677] | Val@20 0.9661  Test@20 0.8783\n",
            "Epoch 161 | loss 0.00181 | lr [0.000674, 0.000674] | Val@20 0.9636  Test@20 0.8764\n",
            "Epoch 162 | loss 0.00176 | lr [0.000670, 0.000670] | Val@20 0.9597  Test@20 0.8769\n",
            "Epoch 163 | loss 0.00176 | lr [0.000666, 0.000666] | Val@20 0.9622  Test@20 0.8787\n",
            "Epoch 164 | loss 0.00175 | lr [0.000662, 0.000662] | Val@20 0.9646  Test@20 0.8870\n",
            "Epoch 165 | loss 0.00178 | lr [0.000658, 0.000658] | Val@20 0.9646  Test@20 0.8909\n",
            "Epoch 166 | loss 0.00172 | lr [0.000655, 0.000655] | Val@20 0.9651  Test@20 0.8827\n",
            "Epoch 167 | loss 0.00175 | lr [0.000651, 0.000651] | Val@20 0.9652  Test@20 0.8850\n",
            "Epoch 168 | loss 0.00165 | lr [0.000647, 0.000647] | Val@20 0.9672  Test@20 0.8861\n",
            "Epoch 169 | loss 0.00164 | lr [0.000643, 0.000643] | Val@20 0.9659  Test@20 0.8867\n",
            "Epoch 170 | loss 0.00165 | lr [0.000639, 0.000639] | Val@20 0.9658  Test@20 0.8921\n",
            "Epoch 171 | loss 0.00169 | lr [0.000635, 0.000635] | Val@20 0.9647  Test@20 0.8851\n",
            "Epoch 172 | loss 0.00168 | lr [0.000631, 0.000631] | Val@20 0.9677  Test@20 0.8772\n",
            "Epoch 173 | loss 0.00165 | lr [0.000627, 0.000627] | Val@20 0.9689  Test@20 0.8912\n",
            "Epoch 174 | loss 0.00159 | lr [0.000624, 0.000624] | Val@20 0.9682  Test@20 0.8794\n",
            "Epoch 175 | loss 0.00165 | lr [0.000620, 0.000620] | Val@20 0.9693  Test@20 0.8910\n",
            "Epoch 176 | loss 0.00162 | lr [0.000616, 0.000616] | Val@20 0.9716  Test@20 0.8827\n",
            "Epoch 177 | loss 0.00158 | lr [0.000612, 0.000612] | Val@20 0.9713  Test@20 0.8921\n",
            "Epoch 178 | loss 0.00160 | lr [0.000608, 0.000608] | Val@20 0.9739  Test@20 0.8825\n",
            "Epoch 179 | loss 0.00156 | lr [0.000604, 0.000604] | Val@20 0.9696  Test@20 0.8897\n",
            "Epoch 180 | loss 0.00153 | lr [0.000600, 0.000600] | Val@20 0.9715  Test@20 0.8874\n",
            "Epoch 181 | loss 0.00153 | lr [0.000596, 0.000596] | Val@20 0.9716  Test@20 0.8916\n",
            "Epoch 182 | loss 0.00152 | lr [0.000592, 0.000592] | Val@20 0.9702  Test@20 0.8954\n",
            "Epoch 183 | loss 0.00151 | lr [0.000588, 0.000588] | Val@20 0.9686  Test@20 0.8909\n",
            "Epoch 184 | loss 0.00148 | lr [0.000584, 0.000584] | Val@20 0.9703  Test@20 0.8901\n",
            "Epoch 185 | loss 0.00149 | lr [0.000580, 0.000580] | Val@20 0.9737  Test@20 0.8933\n",
            "Epoch 186 | loss 0.00149 | lr [0.000576, 0.000576] | Val@20 0.9722  Test@20 0.8901\n",
            "Epoch 187 | loss 0.00146 | lr [0.000572, 0.000572] | Val@20 0.9720  Test@20 0.8915\n",
            "Epoch 188 | loss 0.00142 | lr [0.000568, 0.000568] | Val@20 0.9716  Test@20 0.8865\n",
            "Epoch 189 | loss 0.00143 | lr [0.000564, 0.000564] | Val@20 0.9718  Test@20 0.8947\n",
            "Epoch 190 | loss 0.00141 | lr [0.000560, 0.000560] | Val@20 0.9723  Test@20 0.8910\n",
            "Epoch 191 | loss 0.00139 | lr [0.000556, 0.000556] | Val@20 0.9721  Test@20 0.8860\n",
            "Epoch 192 | loss 0.00136 | lr [0.000552, 0.000552] | Val@20 0.9739  Test@20 0.8900\n",
            "Epoch 193 | loss 0.00137 | lr [0.000548, 0.000548] | Val@20 0.9720  Test@20 0.8959\n",
            "Epoch 194 | loss 0.00140 | lr [0.000544, 0.000544] | Val@20 0.9726  Test@20 0.8910\n",
            "Epoch 195 | loss 0.00132 | lr [0.000540, 0.000540] | Val@20 0.9743  Test@20 0.8936\n",
            "Epoch 196 | loss 0.00133 | lr [0.000536, 0.000536] | Val@20 0.9731  Test@20 0.8884\n",
            "Epoch 197 | loss 0.00134 | lr [0.000532, 0.000532] | Val@20 0.9721  Test@20 0.8863\n",
            "Epoch 198 | loss 0.00129 | lr [0.000528, 0.000528] | Val@20 0.9723  Test@20 0.8907\n",
            "Epoch 199 | loss 0.00133 | lr [0.000524, 0.000524] | Val@20 0.9726  Test@20 0.8925\n",
            "Epoch 200 | loss 0.00132 | lr [0.000520, 0.000520] | Val@20 0.9759  Test@20 0.8919\n",
            "Epoch 201 | loss 0.00132 | lr [0.000516, 0.000516] | Val@20 0.9715  Test@20 0.8968\n",
            "Epoch 202 | loss 0.00126 | lr [0.000512, 0.000512] | Val@20 0.9747  Test@20 0.8859\n",
            "Epoch 203 | loss 0.00127 | lr [0.000508, 0.000508] | Val@20 0.9742  Test@20 0.8922\n",
            "Epoch 204 | loss 0.00125 | lr [0.000504, 0.000504] | Val@20 0.9759  Test@20 0.8909\n",
            "Epoch 205 | loss 0.00125 | lr [0.000500, 0.000500] | Val@20 0.9751  Test@20 0.8905\n",
            "Epoch 206 | loss 0.00125 | lr [0.000496, 0.000496] | Val@20 0.9756  Test@20 0.8912\n",
            "Epoch 207 | loss 0.00125 | lr [0.000492, 0.000492] | Val@20 0.9747  Test@20 0.8954\n",
            "Epoch 208 | loss 0.00129 | lr [0.000488, 0.000488] | Val@20 0.9744  Test@20 0.8962\n",
            "Epoch 209 | loss 0.00123 | lr [0.000484, 0.000484] | Val@20 0.9742  Test@20 0.8954\n",
            "Epoch 210 | loss 0.00124 | lr [0.000480, 0.000480] | Val@20 0.9735  Test@20 0.8980\n",
            "Epoch 211 | loss 0.00126 | lr [0.000476, 0.000476] | Val@20 0.9744  Test@20 0.8889\n",
            "Epoch 212 | loss 0.00118 | lr [0.000472, 0.000472] | Val@20 0.9773  Test@20 0.9017\n",
            "Epoch 213 | loss 0.00120 | lr [0.000468, 0.000468] | Val@20 0.9779  Test@20 0.8957\n",
            "Epoch 214 | loss 0.00121 | lr [0.000464, 0.000464] | Val@20 0.9734  Test@20 0.8949\n",
            "Epoch 215 | loss 0.00121 | lr [0.000460, 0.000460] | Val@20 0.9769  Test@20 0.8984\n",
            "Epoch 216 | loss 0.00116 | lr [0.000456, 0.000456] | Val@20 0.9776  Test@20 0.8987\n",
            "Epoch 217 | loss 0.00112 | lr [0.000452, 0.000452] | Val@20 0.9754  Test@20 0.8936\n",
            "Epoch 218 | loss 0.00113 | lr [0.000448, 0.000448] | Val@20 0.9761  Test@20 0.9027\n",
            "Epoch 219 | loss 0.00113 | lr [0.000444, 0.000444] | Val@20 0.9785  Test@20 0.9018\n",
            "Epoch 220 | loss 0.00115 | lr [0.000440, 0.000440] | Val@20 0.9755  Test@20 0.9018\n",
            "Epoch 221 | loss 0.00112 | lr [0.000436, 0.000436] | Val@20 0.9770  Test@20 0.9041\n",
            "Epoch 222 | loss 0.00113 | lr [0.000432, 0.000432] | Val@20 0.9787  Test@20 0.8980\n",
            "Epoch 223 | loss 0.00111 | lr [0.000428, 0.000428] | Val@20 0.9795  Test@20 0.8977\n",
            "Epoch 224 | loss 0.00108 | lr [0.000424, 0.000424] | Val@20 0.9780  Test@20 0.8969\n",
            "Epoch 225 | loss 0.00113 | lr [0.000420, 0.000420] | Val@20 0.9791  Test@20 0.8953\n",
            "Epoch 226 | loss 0.00114 | lr [0.000416, 0.000416] | Val@20 0.9798  Test@20 0.9032\n",
            "Epoch 227 | loss 0.00112 | lr [0.000412, 0.000412] | Val@20 0.9809  Test@20 0.8924\n",
            "Epoch 228 | loss 0.00111 | lr [0.000408, 0.000408] | Val@20 0.9786  Test@20 0.9057\n",
            "Epoch 229 | loss 0.00109 | lr [0.000404, 0.000404] | Val@20 0.9802  Test@20 0.8969\n",
            "Epoch 230 | loss 0.00110 | lr [0.000400, 0.000400] | Val@20 0.9787  Test@20 0.8928\n",
            "Epoch 231 | loss 0.00106 | lr [0.000396, 0.000396] | Val@20 0.9816  Test@20 0.8993\n",
            "Epoch 232 | loss 0.00104 | lr [0.000392, 0.000392] | Val@20 0.9793  Test@20 0.8939\n",
            "Epoch 233 | loss 0.00106 | lr [0.000388, 0.000388] | Val@20 0.9799  Test@20 0.9051\n",
            "Epoch 234 | loss 0.00104 | lr [0.000384, 0.000384] | Val@20 0.9813  Test@20 0.9008\n",
            "Epoch 235 | loss 0.00104 | lr [0.000380, 0.000380] | Val@20 0.9803  Test@20 0.9034\n",
            "Epoch 236 | loss 0.00103 | lr [0.000376, 0.000376] | Val@20 0.9802  Test@20 0.8953\n",
            "Epoch 237 | loss 0.00106 | lr [0.000373, 0.000373] | Val@20 0.9810  Test@20 0.9001\n",
            "Epoch 238 | loss 0.00103 | lr [0.000369, 0.000369] | Val@20 0.9819  Test@20 0.8965\n",
            "Epoch 239 | loss 0.00103 | lr [0.000365, 0.000365] | Val@20 0.9826  Test@20 0.9012\n",
            "Epoch 240 | loss 0.00102 | lr [0.000361, 0.000361] | Val@20 0.9814  Test@20 0.8960\n",
            "Epoch 241 | loss 0.00098 | lr [0.000357, 0.000357] | Val@20 0.9821  Test@20 0.9020\n",
            "Epoch 242 | loss 0.00097 | lr [0.000353, 0.000353] | Val@20 0.9822  Test@20 0.9017\n",
            "Epoch 243 | loss 0.00101 | lr [0.000349, 0.000349] | Val@20 0.9815  Test@20 0.9027\n",
            "Epoch 244 | loss 0.00096 | lr [0.000345, 0.000345] | Val@20 0.9811  Test@20 0.9041\n",
            "Epoch 245 | loss 0.00096 | lr [0.000342, 0.000342] | Val@20 0.9810  Test@20 0.9019\n",
            "Epoch 246 | loss 0.00095 | lr [0.000338, 0.000338] | Val@20 0.9840  Test@20 0.9065\n",
            "Epoch 247 | loss 0.00095 | lr [0.000334, 0.000334] | Val@20 0.9819  Test@20 0.8988\n",
            "Epoch 248 | loss 0.00095 | lr [0.000330, 0.000330] | Val@20 0.9840  Test@20 0.9091\n",
            "Epoch 249 | loss 0.00096 | lr [0.000326, 0.000326] | Val@20 0.9823  Test@20 0.9041\n",
            "Epoch 250 | loss 0.00094 | lr [0.000323, 0.000323] | Val@20 0.9825  Test@20 0.9063\n",
            "Epoch 251 | loss 0.00095 | lr [0.000319, 0.000319] | Val@20 0.9828  Test@20 0.9047\n",
            "Epoch 252 | loss 0.00094 | lr [0.000315, 0.000315] | Val@20 0.9828  Test@20 0.9034\n",
            "Epoch 253 | loss 0.00093 | lr [0.000311, 0.000311] | Val@20 0.9851  Test@20 0.9003\n",
            "Epoch 254 | loss 0.00094 | lr [0.000308, 0.000308] | Val@20 0.9833  Test@20 0.9044\n",
            "Epoch 255 | loss 0.00093 | lr [0.000304, 0.000304] | Val@20 0.9812  Test@20 0.9045\n",
            "Epoch 256 | loss 0.00092 | lr [0.000300, 0.000300] | Val@20 0.9836  Test@20 0.9061\n",
            "Epoch 257 | loss 0.00091 | lr [0.000297, 0.000297] | Val@20 0.9824  Test@20 0.9027\n",
            "Epoch 258 | loss 0.00092 | lr [0.000293, 0.000293] | Val@20 0.9829  Test@20 0.8972\n",
            "Epoch 259 | loss 0.00092 | lr [0.000289, 0.000289] | Val@20 0.9838  Test@20 0.9054\n",
            "Epoch 260 | loss 0.00093 | lr [0.000286, 0.000286] | Val@20 0.9820  Test@20 0.8984\n",
            "Epoch 261 | loss 0.00092 | lr [0.000282, 0.000282] | Val@20 0.9846  Test@20 0.9060\n",
            "Epoch 262 | loss 0.00089 | lr [0.000278, 0.000278] | Val@20 0.9831  Test@20 0.9012\n",
            "Epoch 263 | loss 0.00088 | lr [0.000275, 0.000275] | Val@20 0.9840  Test@20 0.9043\n",
            "Epoch 264 | loss 0.00089 | lr [0.000271, 0.000271] | Val@20 0.9837  Test@20 0.8994\n",
            "Epoch 265 | loss 0.00088 | lr [0.000268, 0.000268] | Val@20 0.9839  Test@20 0.9019\n",
            "Epoch 266 | loss 0.00085 | lr [0.000264, 0.000264] | Val@20 0.9831  Test@20 0.9092\n",
            "Epoch 267 | loss 0.00086 | lr [0.000261, 0.000261] | Val@20 0.9845  Test@20 0.9101\n",
            "Epoch 268 | loss 0.00083 | lr [0.000257, 0.000257] | Val@20 0.9850  Test@20 0.9065\n",
            "Epoch 269 | loss 0.00086 | lr [0.000253, 0.000253] | Val@20 0.9852  Test@20 0.9079\n",
            "Epoch 270 | loss 0.00084 | lr [0.000250, 0.000250] | Val@20 0.9850  Test@20 0.9084\n",
            "Epoch 271 | loss 0.00083 | lr [0.000247, 0.000247] | Val@20 0.9861  Test@20 0.9090\n",
            "Epoch 272 | loss 0.00084 | lr [0.000243, 0.000243] | Val@20 0.9865  Test@20 0.9060\n",
            "Epoch 273 | loss 0.00082 | lr [0.000240, 0.000240] | Val@20 0.9871  Test@20 0.9128\n",
            "Epoch 274 | loss 0.00082 | lr [0.000236, 0.000236] | Val@20 0.9847  Test@20 0.9133\n",
            "Epoch 275 | loss 0.00081 | lr [0.000233, 0.000233] | Val@20 0.9841  Test@20 0.9169\n",
            "Epoch 276 | loss 0.00084 | lr [0.000229, 0.000229] | Val@20 0.9851  Test@20 0.9101\n",
            "Epoch 277 | loss 0.00082 | lr [0.000226, 0.000226] | Val@20 0.9857  Test@20 0.9116\n",
            "Epoch 278 | loss 0.00081 | lr [0.000223, 0.000223] | Val@20 0.9854  Test@20 0.9089\n",
            "Epoch 279 | loss 0.00080 | lr [0.000219, 0.000219] | Val@20 0.9857  Test@20 0.9098\n",
            "Epoch 280 | loss 0.00077 | lr [0.000216, 0.000216] | Val@20 0.9861  Test@20 0.9066\n",
            "Epoch 281 | loss 0.00079 | lr [0.000213, 0.000213] | Val@20 0.9856  Test@20 0.9093\n",
            "Epoch 282 | loss 0.00081 | lr [0.000209, 0.000209] | Val@20 0.9853  Test@20 0.9038\n",
            "Epoch 283 | loss 0.00079 | lr [0.000206, 0.000206] | Val@20 0.9854  Test@20 0.9079\n",
            "Epoch 284 | loss 0.00078 | lr [0.000203, 0.000203] | Val@20 0.9855  Test@20 0.9060\n",
            "Epoch 285 | loss 0.00076 | lr [0.000200, 0.000200] | Val@20 0.9852  Test@20 0.9085\n",
            "Epoch 286 | loss 0.00076 | lr [0.000196, 0.000196] | Val@20 0.9851  Test@20 0.9106\n",
            "Epoch 287 | loss 0.00078 | lr [0.000193, 0.000193] | Val@20 0.9865  Test@20 0.9120\n",
            "Epoch 288 | loss 0.00075 | lr [0.000190, 0.000190] | Val@20 0.9847  Test@20 0.9139\n",
            "Epoch 289 | loss 0.00075 | lr [0.000187, 0.000187] | Val@20 0.9858  Test@20 0.9101\n",
            "Epoch 290 | loss 0.00076 | lr [0.000184, 0.000184] | Val@20 0.9866  Test@20 0.9173\n",
            "Epoch 291 | loss 0.00074 | lr [0.000181, 0.000181] | Val@20 0.9858  Test@20 0.9151\n",
            "Epoch 292 | loss 0.00074 | lr [0.000178, 0.000178] | Val@20 0.9867  Test@20 0.9041\n",
            "Epoch 293 | loss 0.00074 | lr [0.000175, 0.000175] | Val@20 0.9866  Test@20 0.9057\n",
            "Epoch 294 | loss 0.00075 | lr [0.000171, 0.000171] | Val@20 0.9864  Test@20 0.9139\n",
            "Epoch 295 | loss 0.00072 | lr [0.000168, 0.000168] | Val@20 0.9841  Test@20 0.9161\n",
            "Epoch 296 | loss 0.00073 | lr [0.000165, 0.000165] | Val@20 0.9862  Test@20 0.9118\n",
            "Epoch 297 | loss 0.00073 | lr [0.000162, 0.000162] | Val@20 0.9867  Test@20 0.9100\n",
            "Epoch 298 | loss 0.00072 | lr [0.000159, 0.000159] | Val@20 0.9871  Test@20 0.9080\n",
            "Epoch 299 | loss 0.00071 | lr [0.000157, 0.000157] | Val@20 0.9873  Test@20 0.9122\n",
            "Epoch 300 | loss 0.00072 | lr [0.000154, 0.000154] | Val@20 0.9871  Test@20 0.9129\n",
            "Epoch 301 | loss 0.00072 | lr [0.000151, 0.000151] | Val@20 0.9862  Test@20 0.9141\n",
            "Epoch 302 | loss 0.00073 | lr [0.000148, 0.000148] | Val@20 0.9856  Test@20 0.9169\n",
            "Epoch 303 | loss 0.00071 | lr [0.000145, 0.000145] | Val@20 0.9871  Test@20 0.9142\n",
            "Epoch 304 | loss 0.00070 | lr [0.000142, 0.000142] | Val@20 0.9864  Test@20 0.9120\n",
            "Epoch 305 | loss 0.00072 | lr [0.000139, 0.000139] | Val@20 0.9882  Test@20 0.9126\n",
            "Epoch 306 | loss 0.00072 | lr [0.000137, 0.000137] | Val@20 0.9875  Test@20 0.9147\n",
            "Epoch 307 | loss 0.00068 | lr [0.000134, 0.000134] | Val@20 0.9877  Test@20 0.9135\n",
            "Epoch 308 | loss 0.00068 | lr [0.000131, 0.000131] | Val@20 0.9870  Test@20 0.9166\n",
            "Epoch 309 | loss 0.00068 | lr [0.000128, 0.000128] | Val@20 0.9871  Test@20 0.9171\n",
            "Epoch 310 | loss 0.00069 | lr [0.000126, 0.000126] | Val@20 0.9878  Test@20 0.9168\n",
            "Epoch 311 | loss 0.00069 | lr [0.000123, 0.000123] | Val@20 0.9874  Test@20 0.9168\n",
            "Epoch 312 | loss 0.00068 | lr [0.000120, 0.000120] | Val@20 0.9871  Test@20 0.9169\n",
            "Epoch 313 | loss 0.00066 | lr [0.000118, 0.000118] | Val@20 0.9884  Test@20 0.9145\n",
            "Epoch 314 | loss 0.00066 | lr [0.000115, 0.000115] | Val@20 0.9871  Test@20 0.9177\n",
            "Epoch 315 | loss 0.00069 | lr [0.000113, 0.000113] | Val@20 0.9878  Test@20 0.9171\n",
            "Epoch 316 | loss 0.00066 | lr [0.000110, 0.000110] | Val@20 0.9870  Test@20 0.9165\n",
            "Epoch 317 | loss 0.00067 | lr [0.000108, 0.000108] | Val@20 0.9862  Test@20 0.9140\n",
            "Epoch 318 | loss 0.00066 | lr [0.000105, 0.000105] | Val@20 0.9876  Test@20 0.9145\n",
            "Epoch 319 | loss 0.00066 | lr [0.000103, 0.000103] | Val@20 0.9879  Test@20 0.9143\n",
            "Epoch 320 | loss 0.00065 | lr [0.000100, 0.000100] | Val@20 0.9871  Test@20 0.9165\n",
            "Epoch 321 | loss 0.00065 | lr [0.000098, 0.000098] | Val@20 0.9870  Test@20 0.9207\n",
            "Epoch 322 | loss 0.00064 | lr [0.000095, 0.000095] | Val@20 0.9879  Test@20 0.9163\n",
            "Epoch 323 | loss 0.00066 | lr [0.000093, 0.000093] | Val@20 0.9878  Test@20 0.9173\n",
            "Epoch 324 | loss 0.00067 | lr [0.000091, 0.000091] | Val@20 0.9891  Test@20 0.9175\n",
            "Epoch 325 | loss 0.00065 | lr [0.000089, 0.000089] | Val@20 0.9888  Test@20 0.9197\n",
            "Epoch 326 | loss 0.00064 | lr [0.000086, 0.000086] | Val@20 0.9890  Test@20 0.9225\n",
            "Epoch 327 | loss 0.00066 | lr [0.000084, 0.000084] | Val@20 0.9895  Test@20 0.9193\n",
            "Epoch 328 | loss 0.00063 | lr [0.000082, 0.000082] | Val@20 0.9886  Test@20 0.9198\n",
            "Epoch 329 | loss 0.00062 | lr [0.000080, 0.000080] | Val@20 0.9891  Test@20 0.9179\n",
            "Epoch 330 | loss 0.00064 | lr [0.000077, 0.000077] | Val@20 0.9883  Test@20 0.9165\n",
            "Epoch 331 | loss 0.00064 | lr [0.000075, 0.000075] | Val@20 0.9886  Test@20 0.9144\n",
            "Epoch 332 | loss 0.00063 | lr [0.000073, 0.000073] | Val@20 0.9884  Test@20 0.9196\n",
            "Epoch 333 | loss 0.00062 | lr [0.000071, 0.000071] | Val@20 0.9882  Test@20 0.9131\n",
            "Epoch 334 | loss 0.00064 | lr [0.000069, 0.000069] | Val@20 0.9889  Test@20 0.9166\n",
            "Epoch 335 | loss 0.00063 | lr [0.000067, 0.000067] | Val@20 0.9889  Test@20 0.9221\n",
            "Epoch 336 | loss 0.00063 | lr [0.000065, 0.000065] | Val@20 0.9887  Test@20 0.9174\n",
            "Epoch 337 | loss 0.00061 | lr [0.000063, 0.000063] | Val@20 0.9886  Test@20 0.9176\n",
            "Epoch 338 | loss 0.00063 | lr [0.000061, 0.000061] | Val@20 0.9889  Test@20 0.9182\n",
            "Epoch 339 | loss 0.00060 | lr [0.000059, 0.000059] | Val@20 0.9886  Test@20 0.9192\n",
            "Epoch 340 | loss 0.00063 | lr [0.000057, 0.000057] | Val@20 0.9888  Test@20 0.9170\n",
            "Epoch 341 | loss 0.00060 | lr [0.000055, 0.000055] | Val@20 0.9887  Test@20 0.9190\n",
            "Epoch 342 | loss 0.00062 | lr [0.000054, 0.000054] | Val@20 0.9888  Test@20 0.9155\n",
            "Epoch 343 | loss 0.00061 | lr [0.000052, 0.000052] | Val@20 0.9886  Test@20 0.9145\n",
            "Epoch 344 | loss 0.00060 | lr [0.000050, 0.000050] | Val@20 0.9885  Test@20 0.9184\n",
            "Epoch 345 | loss 0.00059 | lr [0.000048, 0.000048] | Val@20 0.9877  Test@20 0.9175\n",
            "Epoch 346 | loss 0.00061 | lr [0.000047, 0.000047] | Val@20 0.9884  Test@20 0.9184\n",
            "Epoch 347 | loss 0.00059 | lr [0.000045, 0.000045] | Val@20 0.9885  Test@20 0.9181\n",
            "Epoch 348 | loss 0.00061 | lr [0.000043, 0.000043] | Val@20 0.9882  Test@20 0.9168\n",
            "Epoch 349 | loss 0.00061 | lr [0.000042, 0.000042] | Val@20 0.9880  Test@20 0.9188\n",
            "Epoch 350 | loss 0.00060 | lr [0.000040, 0.000040] | Val@20 0.9888  Test@20 0.9193\n",
            "Epoch 351 | loss 0.00058 | lr [0.000038, 0.000038] | Val@20 0.9891  Test@20 0.9190\n",
            "Epoch 352 | loss 0.00060 | lr [0.000037, 0.000037] | Val@20 0.9890  Test@20 0.9199\n",
            "Epoch 353 | loss 0.00061 | lr [0.000035, 0.000035] | Val@20 0.9881  Test@20 0.9193\n",
            "Epoch 354 | loss 0.00060 | lr [0.000034, 0.000034] | Val@20 0.9889  Test@20 0.9190\n",
            "Epoch 355 | loss 0.00059 | lr [0.000032, 0.000032] | Val@20 0.9888  Test@20 0.9178\n",
            "Epoch 356 | loss 0.00058 | lr [0.000031, 0.000031] | Val@20 0.9886  Test@20 0.9179\n",
            "Epoch 357 | loss 0.00062 | lr [0.000030, 0.000030] | Val@20 0.9890  Test@20 0.9173\n",
            "Epoch 358 | loss 0.00056 | lr [0.000028, 0.000028] | Val@20 0.9890  Test@20 0.9162\n",
            "Epoch 359 | loss 0.00059 | lr [0.000027, 0.000027] | Val@20 0.9889  Test@20 0.9160\n",
            "Epoch 360 | loss 0.00060 | lr [0.000026, 0.000026] | Val@20 0.9891  Test@20 0.9157\n",
            "Epoch 361 | loss 0.00057 | lr [0.000024, 0.000024] | Val@20 0.9887  Test@20 0.9159\n",
            "Epoch 362 | loss 0.00059 | lr [0.000023, 0.000023] | Val@20 0.9888  Test@20 0.9173\n",
            "Epoch 363 | loss 0.00057 | lr [0.000022, 0.000022] | Val@20 0.9889  Test@20 0.9186\n",
            "Epoch 364 | loss 0.00060 | lr [0.000021, 0.000021] | Val@20 0.9895  Test@20 0.9187\n",
            "Epoch 365 | loss 0.00057 | lr [0.000020, 0.000020] | Val@20 0.9899  Test@20 0.9202\n",
            "Epoch 366 | loss 0.00057 | lr [0.000019, 0.000019] | Val@20 0.9890  Test@20 0.9204\n",
            "Epoch 367 | loss 0.00056 | lr [0.000018, 0.000018] | Val@20 0.9888  Test@20 0.9198\n",
            "Epoch 368 | loss 0.00058 | lr [0.000017, 0.000017] | Val@20 0.9888  Test@20 0.9201\n",
            "Epoch 369 | loss 0.00059 | lr [0.000016, 0.000016] | Val@20 0.9888  Test@20 0.9190\n",
            "Epoch 370 | loss 0.00057 | lr [0.000015, 0.000015] | Val@20 0.9887  Test@20 0.9182\n",
            "Epoch 371 | loss 0.00057 | lr [0.000014, 0.000014] | Val@20 0.9886  Test@20 0.9193\n",
            "Epoch 372 | loss 0.00059 | lr [0.000013, 0.000013] | Val@20 0.9882  Test@20 0.9187\n",
            "Epoch 373 | loss 0.00058 | lr [0.000012, 0.000012] | Val@20 0.9886  Test@20 0.9196\n",
            "Epoch 374 | loss 0.00056 | lr [0.000011, 0.000011] | Val@20 0.9892  Test@20 0.9190\n",
            "Epoch 375 | loss 0.00057 | lr [0.000010, 0.000010] | Val@20 0.9891  Test@20 0.9191\n",
            "Epoch 376 | loss 0.00057 | lr [0.000009, 0.000009] | Val@20 0.9891  Test@20 0.9191\n",
            "Epoch 377 | loss 0.00058 | lr [0.000009, 0.000009] | Val@20 0.9890  Test@20 0.9189\n",
            "Epoch 378 | loss 0.00058 | lr [0.000008, 0.000008] | Val@20 0.9893  Test@20 0.9190\n",
            "Epoch 379 | loss 0.00057 | lr [0.000007, 0.000007] | Val@20 0.9894  Test@20 0.9200\n",
            "Epoch 380 | loss 0.00058 | lr [0.000006, 0.000006] | Val@20 0.9894  Test@20 0.9204\n",
            "Epoch 381 | loss 0.00057 | lr [0.000006, 0.000006] | Val@20 0.9893  Test@20 0.9203\n",
            "Epoch 382 | loss 0.00057 | lr [0.000005, 0.000005] | Val@20 0.9895  Test@20 0.9202\n",
            "Epoch 383 | loss 0.00055 | lr [0.000005, 0.000005] | Val@20 0.9894  Test@20 0.9194\n",
            "Epoch 384 | loss 0.00057 | lr [0.000004, 0.000004] | Val@20 0.9895  Test@20 0.9193\n",
            "Epoch 385 | loss 0.00056 | lr [0.000004, 0.000004] | Val@20 0.9894  Test@20 0.9190\n",
            "Epoch 386 | loss 0.00056 | lr [0.000003, 0.000003] | Val@20 0.9893  Test@20 0.9191\n",
            "Epoch 387 | loss 0.00056 | lr [0.000003, 0.000003] | Val@20 0.9893  Test@20 0.9193\n",
            "Epoch 388 | loss 0.00055 | lr [0.000002, 0.000002] | Val@20 0.9894  Test@20 0.9192\n",
            "Epoch 389 | loss 0.00059 | lr [0.000002, 0.000002] | Val@20 0.9893  Test@20 0.9188\n",
            "Epoch 390 | loss 0.00056 | lr [0.000002, 0.000002] | Val@20 0.9894  Test@20 0.9186\n",
            "Epoch 391 | loss 0.00056 | lr [0.000001, 0.000001] | Val@20 0.9895  Test@20 0.9186\n",
            "Epoch 392 | loss 0.00057 | lr [0.000001, 0.000001] | Val@20 0.9894  Test@20 0.9187\n",
            "Epoch 393 | loss 0.00057 | lr [0.000001, 0.000001] | Val@20 0.9894  Test@20 0.9187\n",
            "Epoch 394 | loss 0.00055 | lr [0.000001, 0.000001] | Val@20 0.9893  Test@20 0.9189\n",
            "Epoch 395 | loss 0.00058 | lr [0.000000, 0.000000] | Val@20 0.9893  Test@20 0.9188\n",
            "Epoch 396 | loss 0.00056 | lr [0.000000, 0.000000] | Val@20 0.9893  Test@20 0.9188\n",
            "Epoch 397 | loss 0.00057 | lr [0.000000, 0.000000] | Val@20 0.9894  Test@20 0.9189\n",
            "Epoch 398 | loss 0.00055 | lr [0.000000, 0.000000] | Val@20 0.9893  Test@20 0.9189\n",
            "Epoch 399 | loss 0.00059 | lr [0.000000, 0.000000] | Val@20 0.9893  Test@20 0.9189\n",
            "Epoch 400 | loss 0.00058 | lr [0.000000, 0.000000] | Val@20 0.9893  Test@20 0.9189\n",
            "\n",
            "Best: epoch 365 | Val@20 0.9899 | Test@20 0.9202\n",
            "Plot saved to /content/drive/MyDrive/CS145/Week11-GCN_SciBert_Ini/results/4267_Enh4Sage_T26_plot.png\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}