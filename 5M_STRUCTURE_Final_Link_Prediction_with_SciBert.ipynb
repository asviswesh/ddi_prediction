{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPI9AO2sLSXy"
      },
      "source": [
        "# Graph Neural Networks for Link Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaJzlJO2Lcj8"
      },
      "source": [
        "Refer to this [blog post](https://medium.com/@tanishjain/224w-final-project-46c1054f2aa4) for more details!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wipe conflicting installs\n",
        "!pip -q uninstall -y torch torchvision torchaudio torch_geometric torch_scatter torch_sparse torch_cluster torch_spline_conv\n",
        "\n",
        "# Install PyTorch built for CUDA 12.1 (fits Colab GPU)\n",
        "!pip install -q --index-url https://download.pytorch.org/whl/cu121 \\\n",
        "  torch==2.4.0+cu121 torchvision==0.19.0+cu121 torchaudio==2.4.0+cu121\n",
        "\n",
        "# Install PyG and its compiled extensions matching that exact Torch/CUDA\n",
        "!pip install -q torch_geometric==2.5.3 \\\n",
        "  -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "\n",
        "# Optional packages\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "!pip install -q PyDrive\n",
        "!pip install ogb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDfu8UjWsKPu",
        "outputId": "80830c2d-abd5-4fbd-de3c-7c513fbb19e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch_scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch_sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch_cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch_spline_conv as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.12/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.5.0)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.12/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.12/dist-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from outdated>=0.2.0->ogb) (2.32.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.0->ogb) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.0->ogb) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "OGB-ddi Link Prediction with Enh4SAGEConv (Torch 2.6 compatible)\n",
        "================================================================\n",
        "\n",
        "This single-file script integrates the fixes & improvements we discussed:\n",
        "\n",
        "Core fixes (most impactful):\n",
        "  1) NEGATIVE SAMPLING WITHOUT LEAKAGE:\n",
        "     - Build two graphs:\n",
        "         - edge_index_train : train-only (undirected, deduped) for message passing\n",
        "         - edge_index_all   : train ∪ valid ∪ test (undirected, deduped) for negative sampling\n",
        "     - This avoids sampling true edges (from valid/test) as negatives.\n",
        "\n",
        "  2) EVALUATION ROBUST TO GROUPED NEGATIVES:\n",
        "     - Supports both flat and grouped shapes of `edge_neg` in OGB.\n",
        "\n",
        "Training tweaks:\n",
        "  - Cosine decay (no warm restarts) with warmup\n",
        "  - Slightly lower dropout & edge_drop for DDI\n",
        "  - Fewer negatives per positive (neg_ratio=2)\n",
        "  - Freeze node embeddings for the first 10 epochs\n",
        "  - Early stopping by Val@20 (patience=40)\n",
        "  - AMP + grad clipping + AdamW remain\n",
        "  - Proximal regularizer to initial embeddings preserved\n",
        "\n",
        "Preserves:\n",
        "  - Google Drive artifact/result paths & file names\n",
        "  - 512-dim external node embeddings\n",
        "  - Enh4SAGEConv + Enh4SAGEStack w/ residuals, LayerNorm, DropEdge, JK-Max\n",
        "  - Plotting training loss and Hits@20 curves\n",
        "\n",
        "Tested with: Torch 2.6.0+cu124, PyG 2.6.1, ogb 1.3.x in Colab (Aug 2025).\n",
        "\n",
        "Changes made since running out of memory\n",
        "1) Set PYTORCH_CUDA_ALLOC_CONF at the very top;\n",
        "2) Call before eval:\n",
        "   if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "# os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:256\")\n",
        "\n",
        "import sys, math, random, json\n",
        "from pathlib import Path\n",
        "\n",
        "# ----------------------------- Minimal deps ----------------------------------\n",
        "def ensure_pkg(pkg: str, pip_name: str = None):\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "    except Exception:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name or pkg])\n",
        "\n",
        "# We do not pin torch/pyg here; your Colab already has 2.6.0/2.6.1.\n",
        "ensure_pkg(\"ogb\")\n",
        "ensure_pkg(\"networkx\", \"networkx>=3.0\")\n",
        "ensure_pkg(\"pydrive\", \"PyDrive\")\n",
        "\n",
        "import torch\n",
        "# ---- Torch 2.6 fix: default weights_only=True breaks OGB/PyG processed files.\n",
        "#      Force weights_only=False for ALL torch.load calls unless explicitly set.\n",
        "if \"weights_only\" in torch.load.__code__.co_varnames:\n",
        "    _orig_load = torch.load\n",
        "    def _load_compat(*args, **kwargs):\n",
        "        kwargs.setdefault(\"weights_only\", False)\n",
        "        return _orig_load(*args, **kwargs)\n",
        "    torch.load = _load_compat  # monkey-patch early\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import networkx as nx\n",
        "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
        "\n",
        "import torch_geometric as pyg\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import negative_sampling, to_networkx\n",
        "from torch_geometric.data import Data\n",
        "from torch.utils.data import DataLoader  # safer generic loader\n",
        "\n",
        "# ------------------------------ Colab Drive ----------------------------------\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ------------------------------ Paths ----------------------------------------\n",
        "BASE_DIR = \"/content/drive/MyDrive/CS145/neurips/FINAL-CODE/\"\n",
        "ART_EMB_DIR = f\"{BASE_DIR}/artifacts\"\n",
        "ART_DIR  = f\"{BASE_DIR}/artifacts_seed7\"\n",
        "RES_DIR  = f\"{BASE_DIR}/results_seed7\"\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "os.makedirs(RES_DIR, exist_ok=True)\n",
        "\n",
        "EMB_PATH = f\"{ART_EMB_DIR}/projected_embeddings_512.pt\"  # [N,512]\n",
        "SPD_PATH = f\"{ART_DIR}/shortest_paths.pt\"\n",
        "EA_PATH  = f\"{ART_DIR}/edge_attr.pt\"\n",
        "\n",
        "# ------------------------------ Device/seed ----------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"CPU\")\n",
        "print(\"PyG:\", pyg.__version__)\n",
        "\n",
        "def set_all_seeds(seed=7):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_all_seeds(7)\n",
        "os.environ.setdefault(\"PYTORCH_DISABLE_DYNAMO\", \"1\")\n",
        "os.environ.setdefault(\"TORCH_COMPILE_DISABLE\", \"1\")\n",
        "\n",
        "# ------------------------------ Dataset --------------------------------------\n",
        "dataset = PygLinkPropPredDataset(name=\"ogbl-ddi\", root='./dataset/')\n",
        "data_obj = dataset[0]\n",
        "split_edge = dataset.get_edge_split()\n",
        "evaluator = Evaluator(name='ogbl-ddi')\n",
        "\n",
        "num_nodes = data_obj.num_nodes\n",
        "edge_index_raw = data_obj.edge_index.to(device)\n",
        "print(f\"Loaded raw graph: {num_nodes} nodes, {edge_index_raw.size(1)} edges\")\n",
        "\n",
        "# ---------------------------- Build Train/All graphs -------------------------\n",
        "def to_undirected_coalesce(ei: torch.Tensor) -> torch.Tensor:\n",
        "    # Make undirected and dedupe edges.\n",
        "    ei_ud = torch.cat([ei, ei.flip(0)], dim=1)\n",
        "    ei_ud = torch.unique(ei_ud.t(), dim=0).t().contiguous()\n",
        "    return ei_ud\n",
        "\n",
        "train_ei = split_edge['train']['edge'].to(device).t().contiguous()\n",
        "valid_ei = split_edge['valid']['edge'].to(device).t().contiguous()\n",
        "test_ei  = split_edge['test']['edge'].to(device).t().contiguous()\n",
        "\n",
        "edge_index_train = to_undirected_coalesce(train_ei)\n",
        "edge_index_all   = to_undirected_coalesce(torch.cat([train_ei, valid_ei, test_ei], dim=1))\n",
        "\n",
        "print(f\"edge_index_train: {edge_index_train.size(1)} undirected edges\")\n",
        "print(f\"edge_index_all  : {edge_index_all.size(1)} undirected edges (for neg sampling only)\")\n",
        "\n",
        "# ---------------------- Shortest paths & edge attributes ---------------------\n",
        "def get_spd_matrix(G: nx.Graph, anchors, max_spd=5):\n",
        "    spd = np.zeros((G.number_of_nodes(), len(anchors)), dtype=np.float32)\n",
        "    for i, a in enumerate(anchors):\n",
        "        for node, L in nx.shortest_path_length(G, source=int(a)).items():\n",
        "            spd[int(node), i] = min(L, max_spd)\n",
        "    return spd\n",
        "\n",
        "def compute_anchor_distances(num_nodes, edge_index, num_anchors=500, max_path_length=5, device='cpu'):\n",
        "    \"\"\"\n",
        "    Compute SPD to random anchors on the TRAIN graph (avoid leakage).\n",
        "    \"\"\"\n",
        "    np.random.seed(7)\n",
        "    # Build a minimal PyG Data for the train graph\n",
        "    d = Data(num_nodes=num_nodes, edge_index=edge_index)\n",
        "    G = to_networkx(d, to_undirected=True)\n",
        "    anchors = np.random.choice(G.number_of_nodes(), size=min(num_anchors, num_nodes), replace=False)\n",
        "    spd = get_spd_matrix(G, anchors, max_spd=max_path_length)\n",
        "    return torch.tensor(spd, dtype=torch.float32, device=device)  # [N, A]\n",
        "\n",
        "def prepare_edge_attributes(shortest_paths_to_anchors, edge_index, num_samples=5):\n",
        "    \"\"\"\n",
        "    Matches prior construction, but built on TRAIN edges:\n",
        "      - SPD rows for endpoints -> mean -> [E, A]\n",
        "      - For each of S samples, pick 200 anchors, mean -> [E, S]\n",
        "      - Per-column min-max normalize to [0,1]\n",
        "    \"\"\"\n",
        "    E = edge_index.size(1)\n",
        "    N, A = shortest_paths_to_anchors.shape\n",
        "\n",
        "    spa = shortest_paths_to_anchors[edge_index, :].mean(dim=0)  # [E, A]\n",
        "\n",
        "    rng = np.random.default_rng(42)\n",
        "    pick = min(200, A)\n",
        "    masks = np.stack([rng.choice(A, size=pick, replace=False) for _ in range(num_samples)], axis=0)  # [S, pick]\n",
        "    masks_t = torch.tensor(masks, device=spa.device, dtype=torch.long)\n",
        "\n",
        "    ea = spa[:, masks_t].mean(dim=2)  # [E, S]\n",
        "    a_max = ea.max(dim=0, keepdim=True).values\n",
        "    a_min = ea.min(dim=0, keepdim=True).values\n",
        "    ea = (ea - a_min) / (a_max - a_min + 1e-6)\n",
        "    return ea\n",
        "\n",
        "# Always recompute on TRAIN graph to avoid leakage and overwrite previous files.\n",
        "print(\"Computing shortest_paths and edge_attr on TRAIN graph (overwriting any existing files to avoid leakage)...\")\n",
        "shortest_paths = compute_anchor_distances(num_nodes, edge_index_train, num_anchors=500, max_path_length=5, device=device)\n",
        "edge_attr_full = prepare_edge_attributes(shortest_paths, edge_index_train, num_samples=5)\n",
        "torch.save(shortest_paths, SPD_PATH)\n",
        "torch.save(edge_attr_full, EA_PATH)\n",
        "print(f\"Saved shortest_paths -> {SPD_PATH}\")\n",
        "print(f\"Saved edge_attr     -> {EA_PATH}\")\n",
        "print(\"edge_attr shape:\", tuple(edge_attr_full.shape))  # [E_train, S]\n",
        "assert edge_attr_full.dim() == 2\n",
        "\n",
        "# ---------------------------- 512-dim embeddings -----------------------------\n",
        "Z = torch.load(EMB_PATH, map_location=device).float()\n",
        "assert Z.ndim == 2 and Z.shape[1] == 512 and Z.shape[0] == num_nodes, f\"Expected [{num_nodes},512], got {Z.shape}\"\n",
        "\n",
        "emb = nn.Embedding.from_pretrained(Z, freeze=False).to(device)\n",
        "E0  = nn.Embedding(num_nodes, Z.size(1)).to(device)\n",
        "E0.weight.data.copy_(emb.weight.data)   # snapshot of initial features\n",
        "\n",
        "# ----------------------------- Enh4SAGEConv ----------------------------------\n",
        "from typing import Union, Tuple\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
        "\n",
        "class Enh4SAGEConv(MessagePassing):\n",
        "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
        "                 out_channels: int, edge_attr_dim: int, normalize: bool = False,\n",
        "                 root_weight: bool = True, bias: bool = True, **kwargs):\n",
        "        kwargs.setdefault('aggr', 'mean')\n",
        "        super().__init__(**kwargs)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.normalize = normalize\n",
        "        self.root_weight = root_weight\n",
        "        self.edge_attr_dim = edge_attr_dim\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
        "        if self.root_weight:\n",
        "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
        "\n",
        "        self.lin_edge = Linear(edge_attr_dim, in_channels[0], bias=False)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin_l.reset_parameters()\n",
        "        if self.root_weight:\n",
        "            self.lin_r.reset_parameters()\n",
        "        self.lin_edge.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
        "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
        "        if isinstance(x, Tensor):\n",
        "            x = (x, x)\n",
        "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
        "        out = self.lin_l(out)\n",
        "        if self.root_weight and x[1] is not None:\n",
        "            out = out + self.lin_r(x[1])\n",
        "        if self.normalize:\n",
        "            out = F.normalize(out, p=2., dim=-1)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
        "        embedded_edge_attr = self.lin_edge(edge_attr)\n",
        "        return F.relu(x_j + embedded_edge_attr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}({self.in_channels}, {self.out_channels})'\n",
        "\n",
        "# -------------------------- Enhanced SAGE Stack ------------------------------\n",
        "class Enh4SAGEStack(nn.Module):\n",
        "    \"\"\"\n",
        "    Residuals + LayerNorm + DropEdge + JK-Max around Enh4SAGEConv.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
        "                 num_layers, dropout, edge_attr_dim, edge_drop=0.05, jk=\"max\"):\n",
        "        super().__init__()\n",
        "        assert num_layers >= 2\n",
        "        self.edge_drop = edge_drop\n",
        "        self.dropout   = dropout\n",
        "        self.jk        = jk\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.norms = nn.ModuleList()\n",
        "        self.convs.append(Enh4SAGEConv(in_channels, hidden_channels, edge_attr_dim))\n",
        "        self.norms.append(nn.LayerNorm(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(Enh4SAGEConv(hidden_channels, hidden_channels, edge_attr_dim))\n",
        "            self.norms.append(nn.LayerNorm(hidden_channels))\n",
        "        self.convs.append(Enh4SAGEConv(hidden_channels, hidden_channels, edge_attr_dim))\n",
        "        self.norms.append(nn.LayerNorm(hidden_channels))\n",
        "\n",
        "        self.proj = nn.Identity() if hidden_channels == out_channels else nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for c in self.convs:\n",
        "            c.reset_parameters()\n",
        "        for n in self.norms:\n",
        "            if hasattr(n, \"reset_parameters\"):\n",
        "                n.reset_parameters()\n",
        "        if isinstance(self.proj, nn.Linear):\n",
        "            nn.init.xavier_uniform_(self.proj.weight); nn.init.zeros_(self.proj.bias)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, *_):\n",
        "        xs = []\n",
        "        E = edge_index.size(1)\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            if self.training and self.edge_drop > 0:\n",
        "                mask = torch.rand(E, device=edge_index.device) > self.edge_drop\n",
        "                ei = edge_index[:, mask]\n",
        "                ea = edge_attr[mask]\n",
        "            else:\n",
        "                ei, ea = edge_index, edge_attr\n",
        "            h = conv(x, ei, ea)\n",
        "            h = self.norms[i](h)\n",
        "            h = F.relu(h)\n",
        "            if h.shape == x.shape:\n",
        "                h = h + x\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            x = h\n",
        "            xs.append(h)\n",
        "        if self.jk == \"max\":\n",
        "            h_out = torch.stack(xs, dim=0).max(dim=0).values\n",
        "        else:\n",
        "            h_out = xs[-1]\n",
        "        return self.proj(h_out)\n",
        "\n",
        "# ------------------------------ Predictor ------------------------------------\n",
        "class LinkPredictor(nn.Module):\n",
        "    \"\"\"Returns logits.\"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels=512, num_layers=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        f_in = 4 * in_channels  # [hi*hj, |hi-hj|, hi, hj]\n",
        "        layers = []\n",
        "        dim = f_in\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers += [nn.Linear(dim, hidden_channels), nn.ReLU(), nn.Dropout(dropout)]\n",
        "            dim = hidden_channels\n",
        "        layers += [nn.Linear(dim, 1)]\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, hi, hj):\n",
        "        x = torch.cat([hi * hj, torch.abs(hi - hj), hi, hj], dim=-1)\n",
        "        return self.mlp(x).view(-1)  # logits\n",
        "\n",
        "# ------------------------- Eval (Hits@K via OGB) -----------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, predictor, edge_index_msg, edge_attr_msg, x, batch_size, split_edge, evaluator):\n",
        "    model.eval(); predictor.eval()\n",
        "\n",
        "    use_amp = (x.is_cuda)  # or reuse H[\"use_amp\"]\n",
        "    with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "      h = model(x, edge_index_msg, edge_attr_msg, None)\n",
        "\n",
        "    def score_pairs(edge_pairs):\n",
        "        out = []\n",
        "        for perm in DataLoader(range(edge_pairs.size(0)), batch_size=batch_size, shuffle=False):\n",
        "            e = edge_pairs[perm].t()\n",
        "            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "              out.append(torch.sigmoid(predictor(h[e[0]], h[e[1]])).cpu())\n",
        "        return torch.cat(out, dim=0)  # 1D\n",
        "\n",
        "    def score_maybe_grouped(neg):\n",
        "        # neg: either [M,2] or [G, R, 2]\n",
        "        if neg.dim() == 2:\n",
        "            return score_pairs(neg)                         # [M]\n",
        "        else:\n",
        "            G, R, _ = neg.shape\n",
        "            neg = neg.view(G*R, 2)\n",
        "            s = score_pairs(neg).view(G, R)                # [G, R]\n",
        "            return s\n",
        "\n",
        "    pos_valid = split_edge['valid']['edge'].to(h.device)\n",
        "    pos_test  = split_edge['test']['edge'].to(h.device)\n",
        "    neg_valid = split_edge['valid']['edge_neg'].to(h.device)\n",
        "    neg_test  = split_edge['test']['edge_neg'].to(h.device)\n",
        "\n",
        "    pos_valid_pred = score_pairs(pos_valid)\n",
        "    pos_test_pred  = score_pairs(pos_test)\n",
        "    neg_valid_pred = score_maybe_grouped(neg_valid)\n",
        "    neg_test_pred  = score_maybe_grouped(neg_test)\n",
        "\n",
        "    results = {}\n",
        "    for K in [20, 50, 100]:\n",
        "        evaluator.K = K\n",
        "        v = evaluator.eval({'y_pred_pos': pos_valid_pred, 'y_pred_neg': neg_valid_pred})[f'hits@{K}']\n",
        "        t = evaluator.eval({'y_pred_pos': pos_test_pred,  'y_pred_neg': neg_test_pred})[f'hits@{K}']\n",
        "        results[f'Hits@{K}'] = (v, t)\n",
        "    return results\n",
        "\n",
        "# --------------------------- Training primitives -----------------------------\n",
        "def bpr_loss(pos_logits: torch.Tensor, neg_logits: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Pairwise ranking loss. We align negatives into shape [B, R] (truncate/pad if needed).\n",
        "    \"\"\"\n",
        "    B = pos_logits.numel()\n",
        "    N = neg_logits.numel()\n",
        "    if B == 0 or N == 0:\n",
        "        return torch.tensor(0.0, device=pos_logits.device)\n",
        "\n",
        "    R = max(1, N // B)\n",
        "    needed = B * R\n",
        "\n",
        "    if N >= needed:\n",
        "        neg_use = neg_logits[:needed]\n",
        "    else:\n",
        "        reps = needed - N\n",
        "        pad = neg_logits[-1:].repeat(reps)\n",
        "        neg_use = torch.cat([neg_logits, pad], dim=0)\n",
        "\n",
        "    neg_mat = neg_use.view(B, R)                 # [B, R]\n",
        "    pos_mat = pos_logits.view(B, 1)              # [B, 1]\n",
        "    return F.softplus(-(pos_mat - neg_mat)).mean()\n",
        "\n",
        "def maybe_negative_sampling(edge_index, num_nodes, num_neg):\n",
        "    # Prefer 'sparse'; fallback to 'dense' if needed for compatibility.\n",
        "    try:\n",
        "        return negative_sampling(edge_index=edge_index, num_nodes=num_nodes,\n",
        "                                 num_neg_samples=num_neg, method='sparse')\n",
        "    except Exception:\n",
        "        return negative_sampling(edge_index=edge_index, num_nodes=num_nodes,\n",
        "                                 num_neg_samples=num_neg, method='dense')\n",
        "\n",
        "def train_one_epoch(model, predictor, x, E0, edge_index_msg, edge_attr_msg, edge_index_for_negs,\n",
        "                    pos_train_edges, optimizer, scaler, batch_size,\n",
        "                    neg_ratio=2, lam=5e-4, hard_frac=0.5, hard_mul=1.5, use_amp=True):\n",
        "    model.train(); predictor.train()\n",
        "    total_loss, total_examples = 0.0, 0\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    n = pos_train_edges.size(0)\n",
        "    for perm in DataLoader(range(n), batch_size=batch_size, shuffle=True):\n",
        "        idx = torch.as_tensor(perm, dtype=torch.long, device=device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(use_amp and device.type=='cuda')):\n",
        "            h = model(x, edge_index_msg, edge_attr_msg, None)\n",
        "\n",
        "            pos_edge = pos_train_edges[idx].t()     # [2, B]\n",
        "            pos_logits = predictor(h[pos_edge[0]], h[pos_edge[1]])  # [B]\n",
        "\n",
        "            neg_samples = maybe_negative_sampling(\n",
        "                edge_index=edge_index_for_negs,     # <-- ALL positives, to avoid false negatives\n",
        "                num_nodes=x.size(0),\n",
        "                num_neg=pos_logits.numel() * neg_ratio\n",
        "            )\n",
        "            neg_logits = predictor(h[neg_samples[0]], h[neg_samples[1]])  # [B*R]\n",
        "\n",
        "            # Hard-negative mixing\n",
        "            if hard_frac > 0.0 and neg_logits.numel() > 0:\n",
        "                k = max(1, int(hard_frac * neg_logits.numel()))\n",
        "                hard_vals, _ = torch.topk(neg_logits, k=k, largest=True, sorted=False)\n",
        "                extra = hard_vals.repeat_interleave(int(math.ceil(hard_mul)))\n",
        "                neg_logits_eff = torch.cat([neg_logits, extra], dim=0)\n",
        "            else:\n",
        "                neg_logits_eff = neg_logits\n",
        "\n",
        "            loss_rank = bpr_loss(pos_logits, neg_logits_eff)\n",
        "\n",
        "            labels = torch.cat([torch.ones_like(pos_logits), torch.zeros_like(neg_logits)], dim=0)\n",
        "            logits = torch.cat([pos_logits, neg_logits], dim=0)\n",
        "            loss_bce = bce(logits, labels)\n",
        "\n",
        "            touched = torch.unique(torch.cat([pos_edge.reshape(-1), neg_samples.reshape(-1)], dim=0))\n",
        "            prior = (E0(touched) if hasattr(E0, \"__call__\") else E0[touched]).detach()\n",
        "            prox = lam * (x[touched] - prior).pow(2).mean()\n",
        "\n",
        "            loss = loss_rank + 0.15 * loss_bce + prox\n",
        "\n",
        "        if scaler is not None and device.type == \"cuda\" and use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * pos_logits.size(0)\n",
        "        total_examples += pos_logits.size(0)\n",
        "\n",
        "    return total_loss / max(1, total_examples)\n",
        "\n",
        "# ------------------------------- Hyperparams ---------------------------------\n",
        "H = {\n",
        "    \"epochs\":          500,\n",
        "    \"hidden_dim\":      512,\n",
        "    \"dropout\":         0.2,      # slightly lower for DDI\n",
        "    \"num_layers\":      3,\n",
        "    \"lr_main\":         1e-3,\n",
        "    \"lr_emb\":          5e-4,\n",
        "    \"weight_decay\":    0.01,\n",
        "    \"batch_size\":      64 * 1024,\n",
        "    \"neg_ratio\":       2,        # fewer, cleaner negatives\n",
        "    \"lam_prox\":        5e-4,\n",
        "    \"edge_drop\":       0.05,     # gentler drop edge\n",
        "    \"use_amp\":         True,\n",
        "    \"warmup_epochs\":   10,\n",
        "    \"patience\":        40,       # early stopping by Val@20\n",
        "}\n",
        "\n",
        "# ----------------------------- Build + Optimizer -----------------------------\n",
        "edge_attr_full = edge_attr_full.to(device)\n",
        "\n",
        "model = Enh4SAGEStack(\n",
        "    in_channels=emb.embedding_dim,\n",
        "    hidden_channels=H[\"hidden_dim\"],\n",
        "    out_channels=H[\"hidden_dim\"],\n",
        "    num_layers=H[\"num_layers\"],\n",
        "    dropout=H[\"dropout\"],\n",
        "    edge_attr_dim=edge_attr_full.size(1),\n",
        "    edge_drop=H[\"edge_drop\"],\n",
        "    jk=\"max\"\n",
        ").to(device)\n",
        "\n",
        "predictor = LinkPredictor(\n",
        "    in_channels=H[\"hidden_dim\"],\n",
        "    hidden_channels=H[\"hidden_dim\"],\n",
        "    num_layers=3,\n",
        "    dropout=H[\"dropout\"]\n",
        ").to(device)\n",
        "\n",
        "def count_params(m): return sum(p.numel() for p in m.parameters())\n",
        "print(\"Parameters:\")\n",
        "print(\"  GNN       :\", count_params(model))\n",
        "print(\"  Predictor :\", count_params(predictor))\n",
        "print(\"  Embedding :\", count_params(emb))\n",
        "\n",
        "param_groups = [\n",
        "    {\"params\": model.parameters()},\n",
        "    {\"params\": predictor.parameters()},\n",
        "    {\"params\": emb.parameters(), \"lr\": H[\"lr_emb\"]},\n",
        "]\n",
        "optimizer = torch.optim.AdamW(param_groups, lr=H[\"lr_main\"], weight_decay=H[\"weight_decay\"])\n",
        "\n",
        "def lr_lambda_warmup(epoch):\n",
        "    if epoch < H[\"warmup_epochs\"]:\n",
        "        return float(epoch + 1) / float(max(1, H[\"warmup_epochs\"]))\n",
        "    return 1.0\n",
        "\n",
        "warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_warmup)\n",
        "# Cosine decay (no warm restarts)\n",
        "cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=max(1, H[\"epochs\"] - H[\"warmup_epochs\"])\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\" and H[\"use_amp\"]))\n",
        "\n",
        "# --------------------------------- Train ------------------------------------\n",
        "pos_train_edges = split_edge['train']['edge'].to(device)\n",
        "\n",
        "best = {\"val\": 0.0, \"test\": 0.0, \"epoch\": -1}\n",
        "best_test = {\"val\": 0.0, \"test\": 0.0, \"epoch\": -1}\n",
        "train_loss_hist, val_hist, test_hist = [], [], []\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Optional: quick sanity check that false-negative rate is ~0 with ALL graph\n",
        "with torch.no_grad():\n",
        "    # Sample a bunch of negatives against ALL positives (should avoid true edges)\n",
        "    neg_tmp = maybe_negative_sampling(edge_index_all, num_nodes, 200000).t()\n",
        "    # Build a set of ALL positives for membership check\n",
        "    all_pos = torch.unique(edge_index_all.t(), dim=0)\n",
        "    # Check overlap in both directions (undirected)\n",
        "    pos_set = { (int(a), int(b)) for a,b in all_pos.tolist() }\n",
        "    pos_set |= { (b, a) for a,b in all_pos.tolist() }\n",
        "    leak = sum((int(a), int(b)) in pos_set for a,b in neg_tmp.tolist()) / max(1, neg_tmp.size(0))\n",
        "    print(f\"[Sanity] Approx false-negative rate vs ALL positives: {leak:.6f}\")\n",
        "\n",
        "# Freeze node embeddings for warmup (stabilize)\n",
        "emb.requires_grad_(False)\n",
        "\n",
        "for epoch in range(1, H[\"epochs\"] + 1):\n",
        "    x_feats = emb.weight  # trainable features (grad may be disabled initially)\n",
        "\n",
        "    loss = train_one_epoch(\n",
        "        model, predictor, x_feats, E0,\n",
        "        edge_index_msg=edge_index_train, edge_attr_msg=edge_attr_full,\n",
        "        edge_index_for_negs=edge_index_all,\n",
        "        pos_train_edges=pos_train_edges,\n",
        "        optimizer=optimizer, scaler=scaler,\n",
        "        batch_size=H[\"batch_size\"], neg_ratio=H[\"neg_ratio\"], lam=H[\"lam_prox\"],\n",
        "        hard_frac=0.5, hard_mul=1.5, use_amp=H[\"use_amp\"]\n",
        "    )\n",
        "    train_loss_hist.append(loss)\n",
        "\n",
        "    # Unfreeze embeddings after warmup phase\n",
        "    if epoch == H[\"warmup_epochs\"] + 1:\n",
        "        emb.requires_grad_(True)\n",
        "\n",
        "    if epoch <= H[\"warmup_epochs\"]:\n",
        "        warmup.step()\n",
        "    else:\n",
        "        cosine.step()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    results = evaluate(\n",
        "        model, predictor,\n",
        "        edge_index_msg=edge_index_train, edge_attr_msg=edge_attr_full, x=x_feats,\n",
        "        batch_size=H[\"batch_size\"], split_edge=split_edge, evaluator=evaluator\n",
        "    )\n",
        "    val20, test20 = results[\"Hits@20\"]\n",
        "    val_hist.append(val20); test_hist.append(test20)\n",
        "\n",
        "    lr_str = \", \".join([f\"{pg['lr']:.6f}\" for pg in optimizer.param_groups[:2]])\n",
        "    print(f\"Epoch {epoch:03d} | loss {loss:.5f} | lr [{lr_str}] | Val@20 {val20:.4f}  Test@20 {test20:.4f}\")\n",
        "\n",
        "    improved = val20 > best[\"val\"]\n",
        "    improved_test = test20 > best_test[\"test\"]\n",
        "    if improved:\n",
        "        best.update({\"val\": val20, \"test\": test20, \"epoch\": epoch})\n",
        "        print(f\"Best: epoch {best['epoch']} | Val@20 {best['val']:.4f} | Test@20 {best['test']:.4f}\")\n",
        "        # Save best checkpoints\n",
        "        torch.save(model.state_dict(), f\"{ART_DIR}/best_model.pt\")\n",
        "        torch.save(predictor.state_dict(), f\"{ART_DIR}/best_predictor.pt\")\n",
        "        torch.save(emb.state_dict(), f\"{ART_DIR}/best_emb.pt\")\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if improved_test:\n",
        "        best_test.update({\"val\": val20, \"test\": test20, \"epoch\": epoch})\n",
        "        print(f\"Best Test: epoch {best_test['epoch']} | Val@20 {best_test['val']:.4f} | Test@20 {best_test['test']:.4f}\")\n",
        "\n",
        "    if epochs_no_improve >= H[\"patience\"]:\n",
        "        print(f\"Early stopping at epoch {epoch} (no improvement for {H['patience']} epochs).\")\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nBest: epoch {best['epoch']} | Val@20 {best['val']:.4f} | Test@20 {best['test']:.4f}\")\n",
        "print(f\"\\nBest Test: epoch {best_test['epoch']} | Val@20 {best_test['val']:.4f} | Test@20 {best_test['test']:.4f}\")\n",
        "\n",
        "# --------------------------------- Plot -------------------------------------\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title('Link Prediction on OGB-ddi with Enh4SAGEConv (Torch 2.6 fixed)')\n",
        "plt.plot(train_loss_hist, label='train loss')\n",
        "plt.plot(val_hist, label='Hits@20 val')\n",
        "plt.plot(test_hist, label='Hits@20 test')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Metric')\n",
        "plt.grid(True); plt.legend()\n",
        "plot_path = f\"{RES_DIR}/{num_nodes}_Enh4Sage_T26_plot.png\"\n",
        "plt.savefig(plot_path); plt.close()\n",
        "print(f\"Plot saved to {plot_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx5cVLM5ctA2",
        "outputId": "06f1bf65-0609-4fc3-8d02-a8772d750f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cuda\n",
            "Torch: 2.4.0+cu121 CUDA: 12.1\n",
            "PyG: 2.5.3\n",
            "Loaded raw graph: 4267 nodes, 2135822 edges\n",
            "edge_index_train: 2135822 undirected edges\n",
            "edge_index_all  : 2669778 undirected edges (for neg sampling only)\n",
            "Computing shortest_paths and edge_attr on TRAIN graph (overwriting any existing files to avoid leakage)...\n",
            "Saved shortest_paths -> /content/drive/MyDrive/CS145/Week11-GCN_SciBert_Ini//artifacts_seed7/shortest_paths.pt\n",
            "Saved edge_attr     -> /content/drive/MyDrive/CS145/Week11-GCN_SciBert_Ini//artifacts_seed7/edge_attr.pt\n",
            "edge_attr shape: (2135822, 5)\n",
            "Parameters:\n",
            "  GNN       : 1585152\n",
            "  Predictor : 1312257\n",
            "  Embedding : 2184704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1095204342.py:547: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\" and H[\"use_amp\"]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sanity] Approx false-negative rate vs ALL positives: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1095204342.py:433: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(use_amp and device.type=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | loss 0.58087 | lr [0.000200, 0.000200] | Val@20 0.0516  Test@20 0.0810\n",
            "Best: epoch 1 | Val@20 0.0516 | Test@20 0.0810\n",
            "Best Test: epoch 1 | Val@20 0.0516 | Test@20 0.0810\n",
            "Epoch 002 | loss 0.26724 | lr [0.000300, 0.000300] | Val@20 0.4038  Test@20 0.3853\n",
            "Best: epoch 2 | Val@20 0.4038 | Test@20 0.3853\n",
            "Best Test: epoch 2 | Val@20 0.4038 | Test@20 0.3853\n",
            "Epoch 003 | loss 0.10326 | lr [0.000400, 0.000400] | Val@20 0.5010  Test@20 0.5108\n",
            "Best: epoch 3 | Val@20 0.5010 | Test@20 0.5108\n",
            "Best Test: epoch 3 | Val@20 0.5010 | Test@20 0.5108\n",
            "Epoch 004 | loss 0.06011 | lr [0.000500, 0.000500] | Val@20 0.5624  Test@20 0.5407\n",
            "Best: epoch 4 | Val@20 0.5624 | Test@20 0.5407\n",
            "Best Test: epoch 4 | Val@20 0.5624 | Test@20 0.5407\n",
            "Epoch 005 | loss 0.04344 | lr [0.000600, 0.000600] | Val@20 0.5489  Test@20 0.6070\n",
            "Best Test: epoch 5 | Val@20 0.5489 | Test@20 0.6070\n",
            "Epoch 006 | loss 0.03491 | lr [0.000700, 0.000700] | Val@20 0.5870  Test@20 0.6347\n",
            "Best: epoch 6 | Val@20 0.5870 | Test@20 0.6347\n",
            "Best Test: epoch 6 | Val@20 0.5870 | Test@20 0.6347\n",
            "Epoch 007 | loss 0.02999 | lr [0.000800, 0.000800] | Val@20 0.6211  Test@20 0.6619\n",
            "Best: epoch 7 | Val@20 0.6211 | Test@20 0.6619\n",
            "Best Test: epoch 7 | Val@20 0.6211 | Test@20 0.6619\n",
            "Epoch 008 | loss 0.02618 | lr [0.000900, 0.000900] | Val@20 0.6790  Test@20 0.7216\n",
            "Best: epoch 8 | Val@20 0.6790 | Test@20 0.7216\n",
            "Best Test: epoch 8 | Val@20 0.6790 | Test@20 0.7216\n",
            "Epoch 009 | loss 0.02351 | lr [0.001000, 0.001000] | Val@20 0.6812  Test@20 0.7373\n",
            "Best: epoch 9 | Val@20 0.6812 | Test@20 0.7373\n",
            "Best Test: epoch 9 | Val@20 0.6812 | Test@20 0.7373\n",
            "Epoch 010 | loss 0.02215 | lr [0.001000, 0.001000] | Val@20 0.7023  Test@20 0.7393\n",
            "Best: epoch 10 | Val@20 0.7023 | Test@20 0.7393\n",
            "Best Test: epoch 10 | Val@20 0.7023 | Test@20 0.7393\n",
            "Epoch 011 | loss 0.01974 | lr [0.001000, 0.001000] | Val@20 0.7248  Test@20 0.7392\n",
            "Best: epoch 11 | Val@20 0.7248 | Test@20 0.7392\n",
            "Epoch 012 | loss 0.01709 | lr [0.001000, 0.001000] | Val@20 0.7392  Test@20 0.7488\n",
            "Best: epoch 12 | Val@20 0.7392 | Test@20 0.7488\n",
            "Best Test: epoch 12 | Val@20 0.7392 | Test@20 0.7488\n",
            "Epoch 013 | loss 0.01537 | lr [0.001000, 0.001000] | Val@20 0.7514  Test@20 0.7625\n",
            "Best: epoch 13 | Val@20 0.7514 | Test@20 0.7625\n",
            "Best Test: epoch 13 | Val@20 0.7514 | Test@20 0.7625\n",
            "Epoch 014 | loss 0.01405 | lr [0.001000, 0.001000] | Val@20 0.7814  Test@20 0.7758\n",
            "Best: epoch 14 | Val@20 0.7814 | Test@20 0.7758\n",
            "Best Test: epoch 14 | Val@20 0.7814 | Test@20 0.7758\n",
            "Epoch 015 | loss 0.01256 | lr [0.001000, 0.001000] | Val@20 0.8092  Test@20 0.8284\n",
            "Best: epoch 15 | Val@20 0.8092 | Test@20 0.8284\n",
            "Best Test: epoch 15 | Val@20 0.8092 | Test@20 0.8284\n",
            "Epoch 016 | loss 0.01174 | lr [0.001000, 0.001000] | Val@20 0.8031  Test@20 0.8121\n",
            "Epoch 017 | loss 0.01071 | lr [0.000999, 0.000999] | Val@20 0.8286  Test@20 0.8354\n",
            "Best: epoch 17 | Val@20 0.8286 | Test@20 0.8354\n",
            "Best Test: epoch 17 | Val@20 0.8286 | Test@20 0.8354\n",
            "Epoch 018 | loss 0.00990 | lr [0.000999, 0.000999] | Val@20 0.8262  Test@20 0.8449\n",
            "Best Test: epoch 18 | Val@20 0.8262 | Test@20 0.8449\n",
            "Epoch 019 | loss 0.00908 | lr [0.000999, 0.000999] | Val@20 0.8527  Test@20 0.8629\n",
            "Best: epoch 19 | Val@20 0.8527 | Test@20 0.8629\n",
            "Best Test: epoch 19 | Val@20 0.8527 | Test@20 0.8629\n",
            "Epoch 020 | loss 0.00847 | lr [0.000999, 0.000999] | Val@20 0.8387  Test@20 0.8529\n",
            "Epoch 021 | loss 0.00816 | lr [0.000999, 0.000999] | Val@20 0.8637  Test@20 0.8547\n",
            "Best: epoch 21 | Val@20 0.8637 | Test@20 0.8547\n",
            "Epoch 022 | loss 0.00755 | lr [0.000999, 0.000999] | Val@20 0.8723  Test@20 0.8596\n",
            "Best: epoch 22 | Val@20 0.8723 | Test@20 0.8596\n",
            "Epoch 023 | loss 0.00718 | lr [0.000998, 0.000998] | Val@20 0.8608  Test@20 0.8467\n",
            "Epoch 024 | loss 0.00682 | lr [0.000998, 0.000998] | Val@20 0.8682  Test@20 0.8585\n",
            "Epoch 025 | loss 0.00640 | lr [0.000998, 0.000998] | Val@20 0.8789  Test@20 0.8721\n",
            "Best: epoch 25 | Val@20 0.8789 | Test@20 0.8721\n",
            "Best Test: epoch 25 | Val@20 0.8789 | Test@20 0.8721\n",
            "Epoch 026 | loss 0.00621 | lr [0.000997, 0.000997] | Val@20 0.8739  Test@20 0.8633\n",
            "Epoch 027 | loss 0.00605 | lr [0.000997, 0.000997] | Val@20 0.8784  Test@20 0.8663\n",
            "Epoch 028 | loss 0.00567 | lr [0.000997, 0.000997] | Val@20 0.8943  Test@20 0.8831\n",
            "Best: epoch 28 | Val@20 0.8943 | Test@20 0.8831\n",
            "Best Test: epoch 28 | Val@20 0.8943 | Test@20 0.8831\n",
            "Epoch 029 | loss 0.00549 | lr [0.000996, 0.000996] | Val@20 0.8961  Test@20 0.8821\n",
            "Best: epoch 29 | Val@20 0.8961 | Test@20 0.8821\n",
            "Epoch 030 | loss 0.00538 | lr [0.000996, 0.000996] | Val@20 0.8940  Test@20 0.8919\n",
            "Best Test: epoch 30 | Val@20 0.8940 | Test@20 0.8919\n",
            "Epoch 031 | loss 0.00513 | lr [0.000995, 0.000995] | Val@20 0.9081  Test@20 0.8921\n",
            "Best: epoch 31 | Val@20 0.9081 | Test@20 0.8921\n",
            "Best Test: epoch 31 | Val@20 0.9081 | Test@20 0.8921\n",
            "Epoch 032 | loss 0.00508 | lr [0.000995, 0.000995] | Val@20 0.9193  Test@20 0.9037\n",
            "Best: epoch 32 | Val@20 0.9193 | Test@20 0.9037\n",
            "Best Test: epoch 32 | Val@20 0.9193 | Test@20 0.9037\n",
            "Epoch 033 | loss 0.00494 | lr [0.000995, 0.000995] | Val@20 0.9096  Test@20 0.8907\n",
            "Epoch 034 | loss 0.00483 | lr [0.000994, 0.000994] | Val@20 0.9027  Test@20 0.8816\n",
            "Epoch 035 | loss 0.00470 | lr [0.000994, 0.000994] | Val@20 0.9196  Test@20 0.8882\n",
            "Best: epoch 35 | Val@20 0.9196 | Test@20 0.8882\n",
            "Epoch 036 | loss 0.00454 | lr [0.000993, 0.000993] | Val@20 0.9223  Test@20 0.8976\n",
            "Best: epoch 36 | Val@20 0.9223 | Test@20 0.8976\n",
            "Epoch 037 | loss 0.00441 | lr [0.000993, 0.000993] | Val@20 0.9280  Test@20 0.9108\n",
            "Best: epoch 37 | Val@20 0.9280 | Test@20 0.9108\n",
            "Best Test: epoch 37 | Val@20 0.9280 | Test@20 0.9108\n",
            "Epoch 038 | loss 0.00430 | lr [0.000992, 0.000992] | Val@20 0.9272  Test@20 0.9006\n",
            "Epoch 039 | loss 0.00414 | lr [0.000991, 0.000991] | Val@20 0.9300  Test@20 0.9012\n",
            "Best: epoch 39 | Val@20 0.9300 | Test@20 0.9012\n",
            "Epoch 040 | loss 0.00407 | lr [0.000991, 0.000991] | Val@20 0.9286  Test@20 0.8970\n",
            "Epoch 041 | loss 0.00400 | lr [0.000990, 0.000990] | Val@20 0.9273  Test@20 0.9012\n",
            "Epoch 042 | loss 0.00378 | lr [0.000990, 0.000990] | Val@20 0.9228  Test@20 0.9012\n",
            "Epoch 043 | loss 0.00382 | lr [0.000989, 0.000989] | Val@20 0.9365  Test@20 0.9055\n",
            "Best: epoch 43 | Val@20 0.9365 | Test@20 0.9055\n",
            "Epoch 044 | loss 0.00364 | lr [0.000988, 0.000988] | Val@20 0.9419  Test@20 0.9090\n",
            "Best: epoch 44 | Val@20 0.9419 | Test@20 0.9090\n",
            "Epoch 045 | loss 0.00364 | lr [0.000987, 0.000987] | Val@20 0.9373  Test@20 0.9041\n",
            "Epoch 046 | loss 0.00364 | lr [0.000987, 0.000987] | Val@20 0.9366  Test@20 0.9105\n",
            "Epoch 047 | loss 0.00356 | lr [0.000986, 0.000986] | Val@20 0.9358  Test@20 0.9001\n",
            "Epoch 048 | loss 0.00352 | lr [0.000985, 0.000985] | Val@20 0.9391  Test@20 0.9070\n",
            "Epoch 049 | loss 0.00343 | lr [0.000984, 0.000984] | Val@20 0.9325  Test@20 0.9147\n",
            "Best Test: epoch 49 | Val@20 0.9325 | Test@20 0.9147\n",
            "Epoch 050 | loss 0.00335 | lr [0.000984, 0.000984] | Val@20 0.9347  Test@20 0.8963\n",
            "Epoch 051 | loss 0.00332 | lr [0.000983, 0.000983] | Val@20 0.9435  Test@20 0.9150\n",
            "Best: epoch 51 | Val@20 0.9435 | Test@20 0.9150\n",
            "Best Test: epoch 51 | Val@20 0.9435 | Test@20 0.9150\n",
            "Epoch 052 | loss 0.00323 | lr [0.000982, 0.000982] | Val@20 0.9500  Test@20 0.9175\n",
            "Best: epoch 52 | Val@20 0.9500 | Test@20 0.9175\n",
            "Best Test: epoch 52 | Val@20 0.9500 | Test@20 0.9175\n",
            "Epoch 053 | loss 0.00321 | lr [0.000981, 0.000981] | Val@20 0.9511  Test@20 0.9109\n",
            "Best: epoch 53 | Val@20 0.9511 | Test@20 0.9109\n",
            "Epoch 054 | loss 0.00314 | lr [0.000980, 0.000980] | Val@20 0.9492  Test@20 0.9182\n",
            "Best Test: epoch 54 | Val@20 0.9492 | Test@20 0.9182\n",
            "Epoch 055 | loss 0.00303 | lr [0.000979, 0.000979] | Val@20 0.9453  Test@20 0.9040\n",
            "Epoch 056 | loss 0.00298 | lr [0.000978, 0.000978] | Val@20 0.9465  Test@20 0.9091\n",
            "Epoch 057 | loss 0.00298 | lr [0.000977, 0.000977] | Val@20 0.9467  Test@20 0.9076\n",
            "Epoch 058 | loss 0.00296 | lr [0.000977, 0.000977] | Val@20 0.9511  Test@20 0.9156\n",
            "Epoch 059 | loss 0.00294 | lr [0.000976, 0.000976] | Val@20 0.9501  Test@20 0.9033\n",
            "Epoch 060 | loss 0.00297 | lr [0.000975, 0.000975] | Val@20 0.9530  Test@20 0.9080\n",
            "Best: epoch 60 | Val@20 0.9530 | Test@20 0.9080\n",
            "Epoch 061 | loss 0.00286 | lr [0.000974, 0.000974] | Val@20 0.9467  Test@20 0.8982\n",
            "Epoch 062 | loss 0.00276 | lr [0.000972, 0.000972] | Val@20 0.9555  Test@20 0.9012\n",
            "Best: epoch 62 | Val@20 0.9555 | Test@20 0.9012\n",
            "Epoch 063 | loss 0.00274 | lr [0.000971, 0.000971] | Val@20 0.9529  Test@20 0.9014\n",
            "Epoch 064 | loss 0.00269 | lr [0.000970, 0.000970] | Val@20 0.9528  Test@20 0.9053\n",
            "Epoch 065 | loss 0.00257 | lr [0.000969, 0.000969] | Val@20 0.9581  Test@20 0.9100\n",
            "Best: epoch 65 | Val@20 0.9581 | Test@20 0.9100\n",
            "Epoch 066 | loss 0.00251 | lr [0.000968, 0.000968] | Val@20 0.9564  Test@20 0.9157\n",
            "Epoch 067 | loss 0.00253 | lr [0.000967, 0.000967] | Val@20 0.9568  Test@20 0.9139\n",
            "Epoch 068 | loss 0.00255 | lr [0.000966, 0.000966] | Val@20 0.9587  Test@20 0.9107\n",
            "Best: epoch 68 | Val@20 0.9587 | Test@20 0.9107\n",
            "Epoch 069 | loss 0.00246 | lr [0.000965, 0.000965] | Val@20 0.9568  Test@20 0.9179\n",
            "Epoch 070 | loss 0.00238 | lr [0.000963, 0.000963] | Val@20 0.9572  Test@20 0.9151\n",
            "Epoch 071 | loss 0.00235 | lr [0.000962, 0.000962] | Val@20 0.9601  Test@20 0.9180\n",
            "Best: epoch 71 | Val@20 0.9601 | Test@20 0.9180\n",
            "Epoch 072 | loss 0.00231 | lr [0.000961, 0.000961] | Val@20 0.9568  Test@20 0.9149\n",
            "Epoch 073 | loss 0.00242 | lr [0.000960, 0.000960] | Val@20 0.9624  Test@20 0.9129\n",
            "Best: epoch 73 | Val@20 0.9624 | Test@20 0.9129\n",
            "Epoch 074 | loss 0.00240 | lr [0.000958, 0.000958] | Val@20 0.9600  Test@20 0.9058\n",
            "Epoch 075 | loss 0.00229 | lr [0.000957, 0.000957] | Val@20 0.9629  Test@20 0.9175\n",
            "Best: epoch 75 | Val@20 0.9629 | Test@20 0.9175\n",
            "Epoch 076 | loss 0.00228 | lr [0.000956, 0.000956] | Val@20 0.9560  Test@20 0.9157\n",
            "Epoch 077 | loss 0.00226 | lr [0.000955, 0.000955] | Val@20 0.9626  Test@20 0.9186\n",
            "Best Test: epoch 77 | Val@20 0.9626 | Test@20 0.9186\n",
            "Epoch 078 | loss 0.00229 | lr [0.000953, 0.000953] | Val@20 0.9628  Test@20 0.9155\n",
            "Epoch 079 | loss 0.00228 | lr [0.000952, 0.000952] | Val@20 0.9621  Test@20 0.9078\n",
            "Epoch 080 | loss 0.00229 | lr [0.000950, 0.000950] | Val@20 0.9640  Test@20 0.9170\n",
            "Best: epoch 80 | Val@20 0.9640 | Test@20 0.9170\n",
            "Epoch 081 | loss 0.00220 | lr [0.000949, 0.000949] | Val@20 0.9638  Test@20 0.9230\n",
            "Best Test: epoch 81 | Val@20 0.9638 | Test@20 0.9230\n",
            "Epoch 082 | loss 0.00218 | lr [0.000948, 0.000948] | Val@20 0.9653  Test@20 0.9172\n",
            "Best: epoch 82 | Val@20 0.9653 | Test@20 0.9172\n",
            "Epoch 083 | loss 0.00228 | lr [0.000946, 0.000946] | Val@20 0.9649  Test@20 0.9197\n",
            "Epoch 084 | loss 0.00216 | lr [0.000945, 0.000945] | Val@20 0.9656  Test@20 0.9179\n",
            "Best: epoch 84 | Val@20 0.9656 | Test@20 0.9179\n",
            "Epoch 085 | loss 0.00204 | lr [0.000943, 0.000943] | Val@20 0.9655  Test@20 0.9223\n",
            "Epoch 086 | loss 0.00203 | lr [0.000942, 0.000942] | Val@20 0.9652  Test@20 0.9125\n",
            "Epoch 087 | loss 0.00203 | lr [0.000940, 0.000940] | Val@20 0.9606  Test@20 0.9228\n",
            "Epoch 088 | loss 0.00200 | lr [0.000939, 0.000939] | Val@20 0.9585  Test@20 0.9170\n",
            "Epoch 089 | loss 0.00201 | lr [0.000937, 0.000937] | Val@20 0.9612  Test@20 0.9184\n",
            "Epoch 090 | loss 0.00196 | lr [0.000936, 0.000936] | Val@20 0.9577  Test@20 0.9215\n",
            "Epoch 091 | loss 0.00213 | lr [0.000934, 0.000934] | Val@20 0.9642  Test@20 0.9143\n",
            "Epoch 092 | loss 0.00198 | lr [0.000932, 0.000932] | Val@20 0.9663  Test@20 0.9122\n",
            "Best: epoch 92 | Val@20 0.9663 | Test@20 0.9122\n",
            "Epoch 093 | loss 0.00192 | lr [0.000931, 0.000931] | Val@20 0.9682  Test@20 0.9120\n",
            "Best: epoch 93 | Val@20 0.9682 | Test@20 0.9120\n",
            "Epoch 094 | loss 0.00194 | lr [0.000929, 0.000929] | Val@20 0.9672  Test@20 0.9184\n",
            "Epoch 095 | loss 0.00195 | lr [0.000928, 0.000928] | Val@20 0.9668  Test@20 0.9226\n",
            "Epoch 096 | loss 0.00195 | lr [0.000926, 0.000926] | Val@20 0.9645  Test@20 0.9191\n",
            "Epoch 097 | loss 0.00184 | lr [0.000924, 0.000924] | Val@20 0.9632  Test@20 0.9165\n",
            "Epoch 098 | loss 0.00181 | lr [0.000923, 0.000923] | Val@20 0.9642  Test@20 0.9267\n",
            "Best Test: epoch 98 | Val@20 0.9642 | Test@20 0.9267\n",
            "Epoch 099 | loss 0.00188 | lr [0.000921, 0.000921] | Val@20 0.9647  Test@20 0.9205\n",
            "Epoch 100 | loss 0.00179 | lr [0.000919, 0.000919] | Val@20 0.9673  Test@20 0.9203\n",
            "Epoch 101 | loss 0.00177 | lr [0.000917, 0.000917] | Val@20 0.9667  Test@20 0.9265\n",
            "Epoch 102 | loss 0.00176 | lr [0.000916, 0.000916] | Val@20 0.9658  Test@20 0.9177\n",
            "Epoch 103 | loss 0.00176 | lr [0.000914, 0.000914] | Val@20 0.9679  Test@20 0.9290\n",
            "Best Test: epoch 103 | Val@20 0.9679 | Test@20 0.9290\n",
            "Epoch 104 | loss 0.00182 | lr [0.000912, 0.000912] | Val@20 0.9624  Test@20 0.9250\n",
            "Epoch 105 | loss 0.00176 | lr [0.000910, 0.000910] | Val@20 0.9662  Test@20 0.9143\n",
            "Epoch 106 | loss 0.00171 | lr [0.000908, 0.000908] | Val@20 0.9694  Test@20 0.9159\n",
            "Best: epoch 106 | Val@20 0.9694 | Test@20 0.9159\n",
            "Epoch 107 | loss 0.00173 | lr [0.000906, 0.000906] | Val@20 0.9697  Test@20 0.9355\n",
            "Best: epoch 107 | Val@20 0.9697 | Test@20 0.9355\n",
            "Best Test: epoch 107 | Val@20 0.9697 | Test@20 0.9355\n",
            "Epoch 108 | loss 0.00177 | lr [0.000905, 0.000905] | Val@20 0.9623  Test@20 0.9164\n",
            "Epoch 109 | loss 0.00166 | lr [0.000903, 0.000903] | Val@20 0.9687  Test@20 0.9248\n",
            "Epoch 110 | loss 0.00173 | lr [0.000901, 0.000901] | Val@20 0.9652  Test@20 0.9171\n",
            "Epoch 111 | loss 0.00164 | lr [0.000899, 0.000899] | Val@20 0.9720  Test@20 0.9184\n",
            "Best: epoch 111 | Val@20 0.9720 | Test@20 0.9184\n",
            "Epoch 112 | loss 0.00165 | lr [0.000897, 0.000897] | Val@20 0.9649  Test@20 0.9295\n",
            "Epoch 113 | loss 0.00163 | lr [0.000895, 0.000895] | Val@20 0.9700  Test@20 0.9219\n",
            "Epoch 114 | loss 0.00156 | lr [0.000893, 0.000893] | Val@20 0.9699  Test@20 0.9285\n",
            "Epoch 115 | loss 0.00151 | lr [0.000891, 0.000891] | Val@20 0.9709  Test@20 0.9261\n",
            "Epoch 116 | loss 0.00153 | lr [0.000889, 0.000889] | Val@20 0.9726  Test@20 0.9268\n",
            "Best: epoch 116 | Val@20 0.9726 | Test@20 0.9268\n",
            "Epoch 117 | loss 0.00150 | lr [0.000887, 0.000887] | Val@20 0.9742  Test@20 0.9181\n",
            "Best: epoch 117 | Val@20 0.9742 | Test@20 0.9181\n",
            "Epoch 118 | loss 0.00157 | lr [0.000885, 0.000885] | Val@20 0.9759  Test@20 0.9231\n",
            "Best: epoch 118 | Val@20 0.9759 | Test@20 0.9231\n",
            "Epoch 119 | loss 0.00151 | lr [0.000883, 0.000883] | Val@20 0.9757  Test@20 0.9167\n",
            "Epoch 120 | loss 0.00150 | lr [0.000881, 0.000881] | Val@20 0.9735  Test@20 0.9253\n",
            "Epoch 121 | loss 0.00147 | lr [0.000879, 0.000879] | Val@20 0.9753  Test@20 0.9237\n",
            "Epoch 122 | loss 0.00139 | lr [0.000877, 0.000877] | Val@20 0.9731  Test@20 0.9230\n",
            "Epoch 123 | loss 0.00147 | lr [0.000874, 0.000874] | Val@20 0.9766  Test@20 0.9289\n",
            "Best: epoch 123 | Val@20 0.9766 | Test@20 0.9289\n",
            "Epoch 124 | loss 0.00145 | lr [0.000872, 0.000872] | Val@20 0.9758  Test@20 0.9236\n",
            "Epoch 125 | loss 0.00150 | lr [0.000870, 0.000870] | Val@20 0.9742  Test@20 0.9214\n",
            "Epoch 126 | loss 0.00145 | lr [0.000868, 0.000868] | Val@20 0.9768  Test@20 0.9297\n",
            "Best: epoch 126 | Val@20 0.9768 | Test@20 0.9297\n",
            "Epoch 127 | loss 0.00140 | lr [0.000866, 0.000866] | Val@20 0.9772  Test@20 0.9229\n",
            "Best: epoch 127 | Val@20 0.9772 | Test@20 0.9229\n",
            "Epoch 128 | loss 0.00141 | lr [0.000864, 0.000864] | Val@20 0.9799  Test@20 0.9299\n",
            "Best: epoch 128 | Val@20 0.9799 | Test@20 0.9299\n",
            "Epoch 129 | loss 0.00140 | lr [0.000861, 0.000861] | Val@20 0.9761  Test@20 0.9072\n",
            "Epoch 130 | loss 0.00143 | lr [0.000859, 0.000859] | Val@20 0.9775  Test@20 0.9248\n",
            "Epoch 131 | loss 0.00142 | lr [0.000857, 0.000857] | Val@20 0.9738  Test@20 0.9169\n",
            "Epoch 132 | loss 0.00142 | lr [0.000855, 0.000855] | Val@20 0.9779  Test@20 0.9299\n",
            "Epoch 133 | loss 0.00137 | lr [0.000852, 0.000852] | Val@20 0.9783  Test@20 0.9318\n",
            "Epoch 134 | loss 0.00140 | lr [0.000850, 0.000850] | Val@20 0.9768  Test@20 0.9281\n",
            "Epoch 135 | loss 0.00139 | lr [0.000848, 0.000848] | Val@20 0.9787  Test@20 0.9271\n",
            "Epoch 136 | loss 0.00132 | lr [0.000846, 0.000846] | Val@20 0.9784  Test@20 0.9297\n",
            "Epoch 137 | loss 0.00133 | lr [0.000843, 0.000843] | Val@20 0.9768  Test@20 0.9285\n",
            "Epoch 138 | loss 0.00131 | lr [0.000841, 0.000841] | Val@20 0.9777  Test@20 0.9292\n",
            "Epoch 139 | loss 0.00131 | lr [0.000839, 0.000839] | Val@20 0.9783  Test@20 0.9337\n",
            "Epoch 140 | loss 0.00134 | lr [0.000836, 0.000836] | Val@20 0.9764  Test@20 0.9346\n",
            "Epoch 141 | loss 0.00130 | lr [0.000834, 0.000834] | Val@20 0.9746  Test@20 0.9325\n",
            "Epoch 142 | loss 0.00129 | lr [0.000831, 0.000831] | Val@20 0.9788  Test@20 0.9300\n",
            "Epoch 143 | loss 0.00129 | lr [0.000829, 0.000829] | Val@20 0.9774  Test@20 0.9260\n",
            "Epoch 144 | loss 0.00125 | lr [0.000827, 0.000827] | Val@20 0.9743  Test@20 0.9318\n",
            "Epoch 145 | loss 0.00128 | lr [0.000824, 0.000824] | Val@20 0.9800  Test@20 0.9269\n",
            "Best: epoch 145 | Val@20 0.9800 | Test@20 0.9269\n",
            "Epoch 146 | loss 0.00125 | lr [0.000822, 0.000822] | Val@20 0.9781  Test@20 0.9326\n",
            "Epoch 147 | loss 0.00125 | lr [0.000819, 0.000819] | Val@20 0.9772  Test@20 0.9317\n",
            "Epoch 148 | loss 0.00122 | lr [0.000817, 0.000817] | Val@20 0.9766  Test@20 0.9363\n",
            "Best Test: epoch 148 | Val@20 0.9766 | Test@20 0.9363\n",
            "Epoch 149 | loss 0.00127 | lr [0.000814, 0.000814] | Val@20 0.9802  Test@20 0.9253\n",
            "Best: epoch 149 | Val@20 0.9802 | Test@20 0.9253\n",
            "Epoch 150 | loss 0.00121 | lr [0.000812, 0.000812] | Val@20 0.9784  Test@20 0.9236\n",
            "Epoch 151 | loss 0.00121 | lr [0.000809, 0.000809] | Val@20 0.9798  Test@20 0.9296\n",
            "Epoch 152 | loss 0.00126 | lr [0.000807, 0.000807] | Val@20 0.9790  Test@20 0.9358\n",
            "Epoch 153 | loss 0.00122 | lr [0.000804, 0.000804] | Val@20 0.9771  Test@20 0.9305\n",
            "Epoch 154 | loss 0.00119 | lr [0.000802, 0.000802] | Val@20 0.9781  Test@20 0.9375\n",
            "Best Test: epoch 154 | Val@20 0.9781 | Test@20 0.9375\n",
            "Epoch 155 | loss 0.00118 | lr [0.000799, 0.000799] | Val@20 0.9768  Test@20 0.9264\n",
            "Epoch 156 | loss 0.00116 | lr [0.000796, 0.000796] | Val@20 0.9762  Test@20 0.9344\n",
            "Epoch 157 | loss 0.00117 | lr [0.000794, 0.000794] | Val@20 0.9773  Test@20 0.9357\n",
            "Epoch 158 | loss 0.00120 | lr [0.000791, 0.000791] | Val@20 0.9830  Test@20 0.9294\n",
            "Best: epoch 158 | Val@20 0.9830 | Test@20 0.9294\n",
            "Epoch 159 | loss 0.00116 | lr [0.000789, 0.000789] | Val@20 0.9781  Test@20 0.9335\n",
            "Epoch 160 | loss 0.00118 | lr [0.000786, 0.000786] | Val@20 0.9809  Test@20 0.9305\n",
            "Epoch 161 | loss 0.00120 | lr [0.000783, 0.000783] | Val@20 0.9822  Test@20 0.9464\n",
            "Best Test: epoch 161 | Val@20 0.9822 | Test@20 0.9464\n",
            "Epoch 162 | loss 0.00116 | lr [0.000781, 0.000781] | Val@20 0.9812  Test@20 0.9431\n",
            "Epoch 163 | loss 0.00115 | lr [0.000778, 0.000778] | Val@20 0.9828  Test@20 0.9505\n",
            "Best Test: epoch 163 | Val@20 0.9828 | Test@20 0.9505\n",
            "Epoch 164 | loss 0.00108 | lr [0.000775, 0.000775] | Val@20 0.9808  Test@20 0.9374\n",
            "Epoch 165 | loss 0.00108 | lr [0.000773, 0.000773] | Val@20 0.9810  Test@20 0.9290\n",
            "Epoch 166 | loss 0.00112 | lr [0.000770, 0.000770] | Val@20 0.9801  Test@20 0.9403\n",
            "Epoch 167 | loss 0.00106 | lr [0.000767, 0.000767] | Val@20 0.9801  Test@20 0.9395\n",
            "Epoch 168 | loss 0.00103 | lr [0.000765, 0.000765] | Val@20 0.9828  Test@20 0.9360\n",
            "Epoch 169 | loss 0.00105 | lr [0.000762, 0.000762] | Val@20 0.9823  Test@20 0.9368\n",
            "Epoch 170 | loss 0.00106 | lr [0.000759, 0.000759] | Val@20 0.9804  Test@20 0.9362\n",
            "Epoch 171 | loss 0.00103 | lr [0.000756, 0.000756] | Val@20 0.9848  Test@20 0.9318\n",
            "Best: epoch 171 | Val@20 0.9848 | Test@20 0.9318\n",
            "Epoch 172 | loss 0.00105 | lr [0.000754, 0.000754] | Val@20 0.9826  Test@20 0.9372\n",
            "Epoch 173 | loss 0.00102 | lr [0.000751, 0.000751] | Val@20 0.9828  Test@20 0.9423\n",
            "Epoch 174 | loss 0.00100 | lr [0.000748, 0.000748] | Val@20 0.9853  Test@20 0.9267\n",
            "Best: epoch 174 | Val@20 0.9853 | Test@20 0.9267\n",
            "Epoch 175 | loss 0.00102 | lr [0.000745, 0.000745] | Val@20 0.9835  Test@20 0.9335\n",
            "Epoch 176 | loss 0.00104 | lr [0.000743, 0.000743] | Val@20 0.9813  Test@20 0.9397\n",
            "Epoch 177 | loss 0.00106 | lr [0.000740, 0.000740] | Val@20 0.9816  Test@20 0.9366\n",
            "Epoch 178 | loss 0.00100 | lr [0.000737, 0.000737] | Val@20 0.9833  Test@20 0.9359\n",
            "Epoch 179 | loss 0.00105 | lr [0.000734, 0.000734] | Val@20 0.9817  Test@20 0.9277\n",
            "Epoch 180 | loss 0.00103 | lr [0.000731, 0.000731] | Val@20 0.9795  Test@20 0.9286\n",
            "Epoch 181 | loss 0.00107 | lr [0.000728, 0.000728] | Val@20 0.9838  Test@20 0.9379\n",
            "Epoch 182 | loss 0.00100 | lr [0.000726, 0.000726] | Val@20 0.9846  Test@20 0.9393\n",
            "Epoch 183 | loss 0.00097 | lr [0.000723, 0.000723] | Val@20 0.9832  Test@20 0.9323\n",
            "Epoch 184 | loss 0.00100 | lr [0.000720, 0.000720] | Val@20 0.9831  Test@20 0.9347\n",
            "Epoch 185 | loss 0.00099 | lr [0.000717, 0.000717] | Val@20 0.9843  Test@20 0.9264\n",
            "Epoch 186 | loss 0.00094 | lr [0.000714, 0.000714] | Val@20 0.9848  Test@20 0.9352\n",
            "Epoch 187 | loss 0.00097 | lr [0.000711, 0.000711] | Val@20 0.9840  Test@20 0.9313\n",
            "Epoch 188 | loss 0.00095 | lr [0.000708, 0.000708] | Val@20 0.9832  Test@20 0.9451\n",
            "Epoch 189 | loss 0.00092 | lr [0.000705, 0.000705] | Val@20 0.9862  Test@20 0.9404\n",
            "Best: epoch 189 | Val@20 0.9862 | Test@20 0.9404\n",
            "Epoch 190 | loss 0.00090 | lr [0.000702, 0.000702] | Val@20 0.9847  Test@20 0.9383\n",
            "Epoch 191 | loss 0.00091 | lr [0.000699, 0.000699] | Val@20 0.9846  Test@20 0.9246\n",
            "Epoch 192 | loss 0.00093 | lr [0.000697, 0.000697] | Val@20 0.9832  Test@20 0.9454\n",
            "Epoch 193 | loss 0.00093 | lr [0.000694, 0.000694] | Val@20 0.9809  Test@20 0.9357\n",
            "Epoch 194 | loss 0.00092 | lr [0.000691, 0.000691] | Val@20 0.9839  Test@20 0.9315\n",
            "Epoch 195 | loss 0.00089 | lr [0.000688, 0.000688] | Val@20 0.9828  Test@20 0.9378\n",
            "Epoch 196 | loss 0.00090 | lr [0.000685, 0.000685] | Val@20 0.9844  Test@20 0.9262\n",
            "Epoch 197 | loss 0.00096 | lr [0.000682, 0.000682] | Val@20 0.9846  Test@20 0.9396\n",
            "Epoch 198 | loss 0.00091 | lr [0.000679, 0.000679] | Val@20 0.9836  Test@20 0.9480\n",
            "Epoch 199 | loss 0.00091 | lr [0.000676, 0.000676] | Val@20 0.9868  Test@20 0.9380\n",
            "Best: epoch 199 | Val@20 0.9868 | Test@20 0.9380\n",
            "Epoch 200 | loss 0.00092 | lr [0.000673, 0.000673] | Val@20 0.9849  Test@20 0.9462\n",
            "Epoch 201 | loss 0.00090 | lr [0.000670, 0.000670] | Val@20 0.9844  Test@20 0.9543\n",
            "Best Test: epoch 201 | Val@20 0.9844 | Test@20 0.9543\n",
            "Epoch 202 | loss 0.00094 | lr [0.000667, 0.000667] | Val@20 0.9857  Test@20 0.9449\n",
            "Epoch 203 | loss 0.00095 | lr [0.000664, 0.000664] | Val@20 0.9828  Test@20 0.9434\n",
            "Epoch 204 | loss 0.00087 | lr [0.000661, 0.000661] | Val@20 0.9845  Test@20 0.9426\n",
            "Epoch 205 | loss 0.00088 | lr [0.000658, 0.000658] | Val@20 0.9868  Test@20 0.9434\n",
            "Best: epoch 205 | Val@20 0.9868 | Test@20 0.9434\n",
            "Epoch 206 | loss 0.00087 | lr [0.000655, 0.000655] | Val@20 0.9878  Test@20 0.9443\n",
            "Best: epoch 206 | Val@20 0.9878 | Test@20 0.9443\n",
            "Epoch 207 | loss 0.00089 | lr [0.000651, 0.000651] | Val@20 0.9848  Test@20 0.9490\n",
            "Epoch 208 | loss 0.00088 | lr [0.000648, 0.000648] | Val@20 0.9845  Test@20 0.9443\n",
            "Epoch 209 | loss 0.00084 | lr [0.000645, 0.000645] | Val@20 0.9835  Test@20 0.9393\n",
            "Epoch 210 | loss 0.00083 | lr [0.000642, 0.000642] | Val@20 0.9858  Test@20 0.9431\n",
            "Epoch 211 | loss 0.00083 | lr [0.000639, 0.000639] | Val@20 0.9857  Test@20 0.9444\n",
            "Epoch 212 | loss 0.00082 | lr [0.000636, 0.000636] | Val@20 0.9851  Test@20 0.9436\n",
            "Epoch 213 | loss 0.00081 | lr [0.000633, 0.000633] | Val@20 0.9863  Test@20 0.9416\n",
            "Epoch 214 | loss 0.00082 | lr [0.000630, 0.000630] | Val@20 0.9861  Test@20 0.9454\n",
            "Epoch 215 | loss 0.00083 | lr [0.000627, 0.000627] | Val@20 0.9852  Test@20 0.9375\n",
            "Epoch 216 | loss 0.00080 | lr [0.000624, 0.000624] | Val@20 0.9826  Test@20 0.9476\n",
            "Epoch 217 | loss 0.00076 | lr [0.000621, 0.000621] | Val@20 0.9831  Test@20 0.9483\n",
            "Epoch 218 | loss 0.00079 | lr [0.000618, 0.000618] | Val@20 0.9852  Test@20 0.9460\n",
            "Epoch 219 | loss 0.00078 | lr [0.000614, 0.000614] | Val@20 0.9851  Test@20 0.9439\n",
            "Epoch 220 | loss 0.00080 | lr [0.000611, 0.000611] | Val@20 0.9859  Test@20 0.9488\n",
            "Epoch 221 | loss 0.00079 | lr [0.000608, 0.000608] | Val@20 0.9863  Test@20 0.9397\n",
            "Epoch 222 | loss 0.00081 | lr [0.000605, 0.000605] | Val@20 0.9867  Test@20 0.9492\n",
            "Epoch 223 | loss 0.00076 | lr [0.000602, 0.000602] | Val@20 0.9866  Test@20 0.9545\n",
            "Best Test: epoch 223 | Val@20 0.9866 | Test@20 0.9545\n",
            "Epoch 224 | loss 0.00081 | lr [0.000599, 0.000599] | Val@20 0.9868  Test@20 0.9468\n",
            "Epoch 225 | loss 0.00083 | lr [0.000596, 0.000596] | Val@20 0.9867  Test@20 0.9451\n",
            "Epoch 226 | loss 0.00078 | lr [0.000592, 0.000592] | Val@20 0.9866  Test@20 0.9474\n",
            "Epoch 227 | loss 0.00074 | lr [0.000589, 0.000589] | Val@20 0.9840  Test@20 0.9497\n",
            "Epoch 228 | loss 0.00079 | lr [0.000586, 0.000586] | Val@20 0.9877  Test@20 0.9458\n",
            "Epoch 229 | loss 0.00077 | lr [0.000583, 0.000583] | Val@20 0.9862  Test@20 0.9428\n",
            "Epoch 230 | loss 0.00075 | lr [0.000580, 0.000580] | Val@20 0.9868  Test@20 0.9464\n",
            "Epoch 231 | loss 0.00075 | lr [0.000577, 0.000577] | Val@20 0.9857  Test@20 0.9481\n",
            "Epoch 232 | loss 0.00076 | lr [0.000573, 0.000573] | Val@20 0.9893  Test@20 0.9414\n",
            "Best: epoch 232 | Val@20 0.9893 | Test@20 0.9414\n",
            "Epoch 233 | loss 0.00075 | lr [0.000570, 0.000570] | Val@20 0.9882  Test@20 0.9443\n",
            "Epoch 234 | loss 0.00075 | lr [0.000567, 0.000567] | Val@20 0.9879  Test@20 0.9388\n",
            "Epoch 235 | loss 0.00079 | lr [0.000564, 0.000564] | Val@20 0.9871  Test@20 0.9474\n",
            "Epoch 236 | loss 0.00076 | lr [0.000561, 0.000561] | Val@20 0.9876  Test@20 0.9390\n",
            "Epoch 237 | loss 0.00073 | lr [0.000558, 0.000558] | Val@20 0.9864  Test@20 0.9439\n",
            "Epoch 238 | loss 0.00071 | lr [0.000554, 0.000554] | Val@20 0.9868  Test@20 0.9486\n",
            "Epoch 239 | loss 0.00073 | lr [0.000551, 0.000551] | Val@20 0.9879  Test@20 0.9415\n",
            "Epoch 240 | loss 0.00077 | lr [0.000548, 0.000548] | Val@20 0.9875  Test@20 0.9514\n",
            "Epoch 241 | loss 0.00072 | lr [0.000545, 0.000545] | Val@20 0.9905  Test@20 0.9424\n",
            "Best: epoch 241 | Val@20 0.9905 | Test@20 0.9424\n",
            "Epoch 242 | loss 0.00070 | lr [0.000542, 0.000542] | Val@20 0.9881  Test@20 0.9544\n",
            "Epoch 243 | loss 0.00068 | lr [0.000538, 0.000538] | Val@20 0.9881  Test@20 0.9532\n",
            "Epoch 244 | loss 0.00066 | lr [0.000535, 0.000535] | Val@20 0.9882  Test@20 0.9483\n",
            "Epoch 245 | loss 0.00067 | lr [0.000532, 0.000532] | Val@20 0.9870  Test@20 0.9590\n",
            "Best Test: epoch 245 | Val@20 0.9870 | Test@20 0.9590\n",
            "Epoch 246 | loss 0.00069 | lr [0.000529, 0.000529] | Val@20 0.9880  Test@20 0.9448\n",
            "Epoch 247 | loss 0.00072 | lr [0.000526, 0.000526] | Val@20 0.9899  Test@20 0.9420\n",
            "Epoch 248 | loss 0.00071 | lr [0.000522, 0.000522] | Val@20 0.9892  Test@20 0.9423\n",
            "Epoch 249 | loss 0.00073 | lr [0.000519, 0.000519] | Val@20 0.9872  Test@20 0.9532\n",
            "Epoch 250 | loss 0.00071 | lr [0.000516, 0.000516] | Val@20 0.9862  Test@20 0.9536\n",
            "Epoch 251 | loss 0.00067 | lr [0.000513, 0.000513] | Val@20 0.9877  Test@20 0.9488\n",
            "Epoch 252 | loss 0.00067 | lr [0.000510, 0.000510] | Val@20 0.9886  Test@20 0.9557\n",
            "Epoch 253 | loss 0.00065 | lr [0.000506, 0.000506] | Val@20 0.9889  Test@20 0.9486\n",
            "Epoch 254 | loss 0.00065 | lr [0.000503, 0.000503] | Val@20 0.9890  Test@20 0.9556\n",
            "Epoch 255 | loss 0.00069 | lr [0.000500, 0.000500] | Val@20 0.9886  Test@20 0.9511\n",
            "Epoch 256 | loss 0.00067 | lr [0.000497, 0.000497] | Val@20 0.9867  Test@20 0.9550\n",
            "Epoch 257 | loss 0.00066 | lr [0.000494, 0.000494] | Val@20 0.9881  Test@20 0.9535\n",
            "Epoch 258 | loss 0.00069 | lr [0.000490, 0.000490] | Val@20 0.9888  Test@20 0.9404\n",
            "Epoch 259 | loss 0.00065 | lr [0.000487, 0.000487] | Val@20 0.9910  Test@20 0.9469\n",
            "Best: epoch 259 | Val@20 0.9910 | Test@20 0.9469\n",
            "Epoch 260 | loss 0.00069 | lr [0.000484, 0.000484] | Val@20 0.9875  Test@20 0.9466\n",
            "Epoch 261 | loss 0.00070 | lr [0.000481, 0.000481] | Val@20 0.9884  Test@20 0.9466\n",
            "Epoch 262 | loss 0.00067 | lr [0.000478, 0.000478] | Val@20 0.9897  Test@20 0.9533\n",
            "Epoch 263 | loss 0.00069 | lr [0.000474, 0.000474] | Val@20 0.9892  Test@20 0.9442\n",
            "Epoch 264 | loss 0.00065 | lr [0.000471, 0.000471] | Val@20 0.9891  Test@20 0.9562\n",
            "Epoch 265 | loss 0.00065 | lr [0.000468, 0.000468] | Val@20 0.9861  Test@20 0.9480\n",
            "Epoch 266 | loss 0.00065 | lr [0.000465, 0.000465] | Val@20 0.9871  Test@20 0.9427\n",
            "Epoch 267 | loss 0.00064 | lr [0.000462, 0.000462] | Val@20 0.9879  Test@20 0.9543\n",
            "Epoch 268 | loss 0.00063 | lr [0.000458, 0.000458] | Val@20 0.9885  Test@20 0.9522\n",
            "Epoch 269 | loss 0.00062 | lr [0.000455, 0.000455] | Val@20 0.9897  Test@20 0.9496\n",
            "Epoch 270 | loss 0.00064 | lr [0.000452, 0.000452] | Val@20 0.9878  Test@20 0.9599\n",
            "Best Test: epoch 270 | Val@20 0.9878 | Test@20 0.9599\n",
            "Epoch 271 | loss 0.00058 | lr [0.000449, 0.000449] | Val@20 0.9888  Test@20 0.9566\n",
            "Epoch 272 | loss 0.00059 | lr [0.000446, 0.000446] | Val@20 0.9885  Test@20 0.9592\n",
            "Epoch 273 | loss 0.00058 | lr [0.000442, 0.000442] | Val@20 0.9897  Test@20 0.9567\n",
            "Epoch 274 | loss 0.00057 | lr [0.000439, 0.000439] | Val@20 0.9885  Test@20 0.9587\n",
            "Epoch 275 | loss 0.00059 | lr [0.000436, 0.000436] | Val@20 0.9900  Test@20 0.9574\n",
            "Epoch 276 | loss 0.00059 | lr [0.000433, 0.000433] | Val@20 0.9883  Test@20 0.9543\n",
            "Epoch 277 | loss 0.00059 | lr [0.000430, 0.000430] | Val@20 0.9878  Test@20 0.9587\n",
            "Epoch 278 | loss 0.00059 | lr [0.000427, 0.000427] | Val@20 0.9891  Test@20 0.9618\n",
            "Best Test: epoch 278 | Val@20 0.9891 | Test@20 0.9618\n",
            "Epoch 279 | loss 0.00057 | lr [0.000423, 0.000423] | Val@20 0.9881  Test@20 0.9566\n",
            "Epoch 280 | loss 0.00055 | lr [0.000420, 0.000420] | Val@20 0.9890  Test@20 0.9508\n",
            "Epoch 281 | loss 0.00060 | lr [0.000417, 0.000417] | Val@20 0.9898  Test@20 0.9548\n",
            "Epoch 282 | loss 0.00060 | lr [0.000414, 0.000414] | Val@20 0.9887  Test@20 0.9479\n",
            "Epoch 283 | loss 0.00056 | lr [0.000411, 0.000411] | Val@20 0.9903  Test@20 0.9550\n",
            "Epoch 284 | loss 0.00054 | lr [0.000408, 0.000408] | Val@20 0.9914  Test@20 0.9527\n",
            "Best: epoch 284 | Val@20 0.9914 | Test@20 0.9527\n",
            "Epoch 285 | loss 0.00056 | lr [0.000404, 0.000404] | Val@20 0.9896  Test@20 0.9541\n",
            "Epoch 286 | loss 0.00053 | lr [0.000401, 0.000401] | Val@20 0.9900  Test@20 0.9539\n",
            "Epoch 287 | loss 0.00055 | lr [0.000398, 0.000398] | Val@20 0.9904  Test@20 0.9569\n",
            "Epoch 288 | loss 0.00055 | lr [0.000395, 0.000395] | Val@20 0.9905  Test@20 0.9585\n",
            "Epoch 289 | loss 0.00054 | lr [0.000392, 0.000392] | Val@20 0.9900  Test@20 0.9579\n",
            "Epoch 290 | loss 0.00054 | lr [0.000389, 0.000389] | Val@20 0.9900  Test@20 0.9556\n",
            "Epoch 291 | loss 0.00056 | lr [0.000386, 0.000386] | Val@20 0.9904  Test@20 0.9565\n",
            "Epoch 292 | loss 0.00055 | lr [0.000382, 0.000382] | Val@20 0.9909  Test@20 0.9571\n",
            "Epoch 293 | loss 0.00055 | lr [0.000379, 0.000379] | Val@20 0.9917  Test@20 0.9494\n",
            "Best: epoch 293 | Val@20 0.9917 | Test@20 0.9494\n",
            "Epoch 294 | loss 0.00056 | lr [0.000376, 0.000376] | Val@20 0.9892  Test@20 0.9613\n",
            "Epoch 295 | loss 0.00056 | lr [0.000373, 0.000373] | Val@20 0.9908  Test@20 0.9484\n",
            "Epoch 296 | loss 0.00054 | lr [0.000370, 0.000370] | Val@20 0.9916  Test@20 0.9517\n",
            "Epoch 297 | loss 0.00053 | lr [0.000367, 0.000367] | Val@20 0.9904  Test@20 0.9552\n",
            "Epoch 298 | loss 0.00051 | lr [0.000364, 0.000364] | Val@20 0.9898  Test@20 0.9620\n",
            "Best Test: epoch 298 | Val@20 0.9898 | Test@20 0.9620\n",
            "Epoch 299 | loss 0.00052 | lr [0.000361, 0.000361] | Val@20 0.9909  Test@20 0.9585\n",
            "Epoch 300 | loss 0.00052 | lr [0.000358, 0.000358] | Val@20 0.9905  Test@20 0.9537\n",
            "Epoch 301 | loss 0.00051 | lr [0.000355, 0.000355] | Val@20 0.9911  Test@20 0.9580\n",
            "Epoch 302 | loss 0.00051 | lr [0.000352, 0.000352] | Val@20 0.9913  Test@20 0.9563\n",
            "Epoch 303 | loss 0.00059 | lr [0.000349, 0.000349] | Val@20 0.9906  Test@20 0.9582\n",
            "Epoch 304 | loss 0.00054 | lr [0.000345, 0.000345] | Val@20 0.9914  Test@20 0.9581\n",
            "Epoch 305 | loss 0.00051 | lr [0.000342, 0.000342] | Val@20 0.9912  Test@20 0.9568\n",
            "Epoch 306 | loss 0.00048 | lr [0.000339, 0.000339] | Val@20 0.9914  Test@20 0.9582\n",
            "Epoch 307 | loss 0.00048 | lr [0.000336, 0.000336] | Val@20 0.9901  Test@20 0.9599\n",
            "Epoch 308 | loss 0.00048 | lr [0.000333, 0.000333] | Val@20 0.9896  Test@20 0.9582\n",
            "Epoch 309 | loss 0.00048 | lr [0.000330, 0.000330] | Val@20 0.9918  Test@20 0.9576\n",
            "Best: epoch 309 | Val@20 0.9918 | Test@20 0.9576\n",
            "Epoch 310 | loss 0.00047 | lr [0.000327, 0.000327] | Val@20 0.9914  Test@20 0.9552\n",
            "Epoch 311 | loss 0.00050 | lr [0.000324, 0.000324] | Val@20 0.9916  Test@20 0.9592\n",
            "Epoch 312 | loss 0.00047 | lr [0.000321, 0.000321] | Val@20 0.9915  Test@20 0.9566\n",
            "Epoch 313 | loss 0.00052 | lr [0.000318, 0.000318] | Val@20 0.9900  Test@20 0.9525\n",
            "Epoch 314 | loss 0.00049 | lr [0.000315, 0.000315] | Val@20 0.9911  Test@20 0.9629\n",
            "Best Test: epoch 314 | Val@20 0.9911 | Test@20 0.9629\n",
            "Epoch 315 | loss 0.00048 | lr [0.000312, 0.000312] | Val@20 0.9922  Test@20 0.9551\n",
            "Best: epoch 315 | Val@20 0.9922 | Test@20 0.9551\n",
            "Epoch 316 | loss 0.00049 | lr [0.000309, 0.000309] | Val@20 0.9911  Test@20 0.9547\n",
            "Epoch 317 | loss 0.00048 | lr [0.000306, 0.000306] | Val@20 0.9916  Test@20 0.9631\n",
            "Best Test: epoch 317 | Val@20 0.9916 | Test@20 0.9631\n",
            "Epoch 318 | loss 0.00046 | lr [0.000303, 0.000303] | Val@20 0.9915  Test@20 0.9599\n",
            "Epoch 319 | loss 0.00046 | lr [0.000301, 0.000301] | Val@20 0.9904  Test@20 0.9655\n",
            "Best Test: epoch 319 | Val@20 0.9904 | Test@20 0.9655\n",
            "Epoch 320 | loss 0.00045 | lr [0.000298, 0.000298] | Val@20 0.9904  Test@20 0.9606\n",
            "Epoch 321 | loss 0.00044 | lr [0.000295, 0.000295] | Val@20 0.9905  Test@20 0.9626\n",
            "Epoch 322 | loss 0.00045 | lr [0.000292, 0.000292] | Val@20 0.9913  Test@20 0.9603\n",
            "Epoch 323 | loss 0.00051 | lr [0.000289, 0.000289] | Val@20 0.9912  Test@20 0.9632\n",
            "Epoch 324 | loss 0.00046 | lr [0.000286, 0.000286] | Val@20 0.9909  Test@20 0.9587\n",
            "Epoch 325 | loss 0.00045 | lr [0.000283, 0.000283] | Val@20 0.9912  Test@20 0.9586\n",
            "Epoch 326 | loss 0.00045 | lr [0.000280, 0.000280] | Val@20 0.9918  Test@20 0.9605\n",
            "Epoch 327 | loss 0.00046 | lr [0.000277, 0.000277] | Val@20 0.9919  Test@20 0.9586\n",
            "Epoch 328 | loss 0.00043 | lr [0.000274, 0.000274] | Val@20 0.9915  Test@20 0.9538\n",
            "Epoch 329 | loss 0.00044 | lr [0.000272, 0.000272] | Val@20 0.9916  Test@20 0.9626\n",
            "Epoch 330 | loss 0.00043 | lr [0.000269, 0.000269] | Val@20 0.9914  Test@20 0.9609\n",
            "Epoch 331 | loss 0.00043 | lr [0.000266, 0.000266] | Val@20 0.9920  Test@20 0.9659\n",
            "Best Test: epoch 331 | Val@20 0.9920 | Test@20 0.9659\n",
            "Epoch 332 | loss 0.00043 | lr [0.000263, 0.000263] | Val@20 0.9926  Test@20 0.9648\n",
            "Best: epoch 332 | Val@20 0.9926 | Test@20 0.9648\n",
            "Epoch 333 | loss 0.00042 | lr [0.000260, 0.000260] | Val@20 0.9923  Test@20 0.9606\n",
            "Epoch 334 | loss 0.00043 | lr [0.000257, 0.000257] | Val@20 0.9921  Test@20 0.9588\n",
            "Epoch 335 | loss 0.00041 | lr [0.000255, 0.000255] | Val@20 0.9917  Test@20 0.9566\n",
            "Epoch 336 | loss 0.00043 | lr [0.000252, 0.000252] | Val@20 0.9913  Test@20 0.9584\n",
            "Epoch 337 | loss 0.00044 | lr [0.000249, 0.000249] | Val@20 0.9926  Test@20 0.9622\n",
            "Best: epoch 337 | Val@20 0.9926 | Test@20 0.9622\n",
            "Epoch 338 | loss 0.00044 | lr [0.000246, 0.000246] | Val@20 0.9916  Test@20 0.9626\n",
            "Epoch 339 | loss 0.00043 | lr [0.000244, 0.000244] | Val@20 0.9912  Test@20 0.9592\n",
            "Epoch 340 | loss 0.00040 | lr [0.000241, 0.000241] | Val@20 0.9922  Test@20 0.9598\n",
            "Epoch 341 | loss 0.00042 | lr [0.000238, 0.000238] | Val@20 0.9922  Test@20 0.9667\n",
            "Best Test: epoch 341 | Val@20 0.9922 | Test@20 0.9667\n",
            "Epoch 342 | loss 0.00043 | lr [0.000235, 0.000235] | Val@20 0.9923  Test@20 0.9619\n",
            "Epoch 343 | loss 0.00043 | lr [0.000233, 0.000233] | Val@20 0.9926  Test@20 0.9635\n",
            "Epoch 344 | loss 0.00043 | lr [0.000230, 0.000230] | Val@20 0.9921  Test@20 0.9618\n",
            "Epoch 345 | loss 0.00042 | lr [0.000227, 0.000227] | Val@20 0.9916  Test@20 0.9623\n",
            "Epoch 346 | loss 0.00041 | lr [0.000225, 0.000225] | Val@20 0.9917  Test@20 0.9617\n",
            "Epoch 347 | loss 0.00042 | lr [0.000222, 0.000222] | Val@20 0.9920  Test@20 0.9590\n",
            "Epoch 348 | loss 0.00043 | lr [0.000219, 0.000219] | Val@20 0.9931  Test@20 0.9650\n",
            "Best: epoch 348 | Val@20 0.9931 | Test@20 0.9650\n",
            "Epoch 349 | loss 0.00042 | lr [0.000217, 0.000217] | Val@20 0.9932  Test@20 0.9634\n",
            "Best: epoch 349 | Val@20 0.9932 | Test@20 0.9634\n",
            "Epoch 350 | loss 0.00046 | lr [0.000214, 0.000214] | Val@20 0.9916  Test@20 0.9614\n",
            "Epoch 351 | loss 0.00042 | lr [0.000211, 0.000211] | Val@20 0.9929  Test@20 0.9606\n",
            "Epoch 352 | loss 0.00042 | lr [0.000209, 0.000209] | Val@20 0.9926  Test@20 0.9633\n",
            "Epoch 353 | loss 0.00042 | lr [0.000206, 0.000206] | Val@20 0.9920  Test@20 0.9621\n",
            "Epoch 354 | loss 0.00042 | lr [0.000204, 0.000204] | Val@20 0.9925  Test@20 0.9649\n",
            "Epoch 355 | loss 0.00040 | lr [0.000201, 0.000201] | Val@20 0.9915  Test@20 0.9635\n",
            "Epoch 356 | loss 0.00038 | lr [0.000198, 0.000198] | Val@20 0.9928  Test@20 0.9665\n",
            "Epoch 357 | loss 0.00039 | lr [0.000196, 0.000196] | Val@20 0.9931  Test@20 0.9616\n",
            "Epoch 358 | loss 0.00040 | lr [0.000193, 0.000193] | Val@20 0.9927  Test@20 0.9620\n",
            "Epoch 359 | loss 0.00040 | lr [0.000191, 0.000191] | Val@20 0.9924  Test@20 0.9637\n",
            "Epoch 360 | loss 0.00041 | lr [0.000188, 0.000188] | Val@20 0.9920  Test@20 0.9626\n",
            "Epoch 361 | loss 0.00038 | lr [0.000186, 0.000186] | Val@20 0.9927  Test@20 0.9634\n",
            "Epoch 362 | loss 0.00036 | lr [0.000183, 0.000183] | Val@20 0.9927  Test@20 0.9661\n",
            "Epoch 363 | loss 0.00037 | lr [0.000181, 0.000181] | Val@20 0.9922  Test@20 0.9691\n",
            "Best Test: epoch 363 | Val@20 0.9922 | Test@20 0.9691\n",
            "Epoch 364 | loss 0.00036 | lr [0.000178, 0.000178] | Val@20 0.9926  Test@20 0.9668\n",
            "Epoch 365 | loss 0.00036 | lr [0.000176, 0.000176] | Val@20 0.9934  Test@20 0.9626\n",
            "Best: epoch 365 | Val@20 0.9934 | Test@20 0.9626\n",
            "Epoch 366 | loss 0.00038 | lr [0.000173, 0.000173] | Val@20 0.9927  Test@20 0.9667\n",
            "Epoch 367 | loss 0.00036 | lr [0.000171, 0.000171] | Val@20 0.9920  Test@20 0.9667\n",
            "Epoch 368 | loss 0.00037 | lr [0.000169, 0.000169] | Val@20 0.9928  Test@20 0.9653\n",
            "Epoch 369 | loss 0.00037 | lr [0.000166, 0.000166] | Val@20 0.9932  Test@20 0.9655\n",
            "Epoch 370 | loss 0.00035 | lr [0.000164, 0.000164] | Val@20 0.9937  Test@20 0.9678\n",
            "Best: epoch 370 | Val@20 0.9937 | Test@20 0.9678\n",
            "Epoch 371 | loss 0.00037 | lr [0.000161, 0.000161] | Val@20 0.9927  Test@20 0.9652\n",
            "Epoch 372 | loss 0.00037 | lr [0.000159, 0.000159] | Val@20 0.9926  Test@20 0.9650\n",
            "Epoch 373 | loss 0.00037 | lr [0.000157, 0.000157] | Val@20 0.9929  Test@20 0.9667\n",
            "Epoch 374 | loss 0.00036 | lr [0.000154, 0.000154] | Val@20 0.9918  Test@20 0.9687\n",
            "Epoch 375 | loss 0.00037 | lr [0.000152, 0.000152] | Val@20 0.9924  Test@20 0.9658\n",
            "Epoch 376 | loss 0.00035 | lr [0.000150, 0.000150] | Val@20 0.9928  Test@20 0.9671\n",
            "Epoch 377 | loss 0.00036 | lr [0.000148, 0.000148] | Val@20 0.9930  Test@20 0.9667\n",
            "Epoch 378 | loss 0.00035 | lr [0.000145, 0.000145] | Val@20 0.9924  Test@20 0.9681\n",
            "Epoch 379 | loss 0.00036 | lr [0.000143, 0.000143] | Val@20 0.9924  Test@20 0.9681\n",
            "Epoch 380 | loss 0.00036 | lr [0.000141, 0.000141] | Val@20 0.9926  Test@20 0.9658\n",
            "Epoch 381 | loss 0.00035 | lr [0.000139, 0.000139] | Val@20 0.9926  Test@20 0.9689\n",
            "Epoch 382 | loss 0.00035 | lr [0.000136, 0.000136] | Val@20 0.9931  Test@20 0.9655\n",
            "Epoch 383 | loss 0.00033 | lr [0.000134, 0.000134] | Val@20 0.9929  Test@20 0.9689\n",
            "Epoch 384 | loss 0.00034 | lr [0.000132, 0.000132] | Val@20 0.9931  Test@20 0.9684\n",
            "Epoch 385 | loss 0.00034 | lr [0.000130, 0.000130] | Val@20 0.9934  Test@20 0.9697\n",
            "Best Test: epoch 385 | Val@20 0.9934 | Test@20 0.9697\n",
            "Epoch 386 | loss 0.00033 | lr [0.000128, 0.000128] | Val@20 0.9929  Test@20 0.9705\n",
            "Best Test: epoch 386 | Val@20 0.9929 | Test@20 0.9705\n",
            "Epoch 387 | loss 0.00034 | lr [0.000126, 0.000126] | Val@20 0.9935  Test@20 0.9678\n",
            "Epoch 388 | loss 0.00033 | lr [0.000123, 0.000123] | Val@20 0.9937  Test@20 0.9685\n",
            "Epoch 389 | loss 0.00033 | lr [0.000121, 0.000121] | Val@20 0.9938  Test@20 0.9651\n",
            "Best: epoch 389 | Val@20 0.9938 | Test@20 0.9651\n",
            "Epoch 390 | loss 0.00035 | lr [0.000119, 0.000119] | Val@20 0.9928  Test@20 0.9640\n",
            "Epoch 391 | loss 0.00034 | lr [0.000117, 0.000117] | Val@20 0.9938  Test@20 0.9636\n",
            "Epoch 392 | loss 0.00033 | lr [0.000115, 0.000115] | Val@20 0.9928  Test@20 0.9674\n",
            "Epoch 393 | loss 0.00033 | lr [0.000113, 0.000113] | Val@20 0.9932  Test@20 0.9677\n",
            "Epoch 394 | loss 0.00033 | lr [0.000111, 0.000111] | Val@20 0.9930  Test@20 0.9699\n",
            "Epoch 395 | loss 0.00033 | lr [0.000109, 0.000109] | Val@20 0.9933  Test@20 0.9695\n",
            "Epoch 396 | loss 0.00034 | lr [0.000107, 0.000107] | Val@20 0.9934  Test@20 0.9663\n",
            "Epoch 397 | loss 0.00034 | lr [0.000105, 0.000105] | Val@20 0.9930  Test@20 0.9621\n",
            "Epoch 398 | loss 0.00033 | lr [0.000103, 0.000103] | Val@20 0.9936  Test@20 0.9676\n",
            "Epoch 399 | loss 0.00031 | lr [0.000101, 0.000101] | Val@20 0.9934  Test@20 0.9670\n",
            "Epoch 400 | loss 0.00032 | lr [0.000099, 0.000099] | Val@20 0.9936  Test@20 0.9687\n",
            "Epoch 401 | loss 0.00032 | lr [0.000097, 0.000097] | Val@20 0.9931  Test@20 0.9693\n",
            "Epoch 402 | loss 0.00032 | lr [0.000095, 0.000095] | Val@20 0.9939  Test@20 0.9702\n",
            "Best: epoch 402 | Val@20 0.9939 | Test@20 0.9702\n",
            "Epoch 403 | loss 0.00030 | lr [0.000094, 0.000094] | Val@20 0.9937  Test@20 0.9660\n",
            "Epoch 404 | loss 0.00032 | lr [0.000092, 0.000092] | Val@20 0.9939  Test@20 0.9673\n",
            "Epoch 405 | loss 0.00033 | lr [0.000090, 0.000090] | Val@20 0.9940  Test@20 0.9643\n",
            "Best: epoch 405 | Val@20 0.9940 | Test@20 0.9643\n",
            "Epoch 406 | loss 0.00032 | lr [0.000088, 0.000088] | Val@20 0.9930  Test@20 0.9691\n",
            "Epoch 407 | loss 0.00034 | lr [0.000086, 0.000086] | Val@20 0.9936  Test@20 0.9671\n",
            "Epoch 408 | loss 0.00031 | lr [0.000084, 0.000084] | Val@20 0.9938  Test@20 0.9653\n",
            "Epoch 409 | loss 0.00032 | lr [0.000083, 0.000083] | Val@20 0.9939  Test@20 0.9683\n",
            "Epoch 410 | loss 0.00032 | lr [0.000081, 0.000081] | Val@20 0.9937  Test@20 0.9669\n",
            "Epoch 411 | loss 0.00031 | lr [0.000079, 0.000079] | Val@20 0.9934  Test@20 0.9681\n",
            "Epoch 412 | loss 0.00032 | lr [0.000077, 0.000077] | Val@20 0.9944  Test@20 0.9634\n",
            "Best: epoch 412 | Val@20 0.9944 | Test@20 0.9634\n",
            "Epoch 413 | loss 0.00031 | lr [0.000076, 0.000076] | Val@20 0.9943  Test@20 0.9672\n",
            "Epoch 414 | loss 0.00030 | lr [0.000074, 0.000074] | Val@20 0.9938  Test@20 0.9673\n",
            "Epoch 415 | loss 0.00031 | lr [0.000072, 0.000072] | Val@20 0.9945  Test@20 0.9660\n",
            "Best: epoch 415 | Val@20 0.9945 | Test@20 0.9660\n",
            "Epoch 416 | loss 0.00031 | lr [0.000071, 0.000071] | Val@20 0.9941  Test@20 0.9681\n",
            "Epoch 417 | loss 0.00032 | lr [0.000069, 0.000069] | Val@20 0.9941  Test@20 0.9642\n",
            "Epoch 418 | loss 0.00031 | lr [0.000068, 0.000068] | Val@20 0.9940  Test@20 0.9670\n",
            "Epoch 419 | loss 0.00030 | lr [0.000066, 0.000066] | Val@20 0.9935  Test@20 0.9665\n",
            "Epoch 420 | loss 0.00031 | lr [0.000064, 0.000064] | Val@20 0.9938  Test@20 0.9661\n",
            "Epoch 421 | loss 0.00031 | lr [0.000063, 0.000063] | Val@20 0.9940  Test@20 0.9652\n",
            "Epoch 422 | loss 0.00031 | lr [0.000061, 0.000061] | Val@20 0.9942  Test@20 0.9675\n",
            "Epoch 423 | loss 0.00030 | lr [0.000060, 0.000060] | Val@20 0.9938  Test@20 0.9653\n",
            "Epoch 424 | loss 0.00030 | lr [0.000058, 0.000058] | Val@20 0.9939  Test@20 0.9674\n",
            "Epoch 425 | loss 0.00029 | lr [0.000057, 0.000057] | Val@20 0.9940  Test@20 0.9680\n",
            "Epoch 426 | loss 0.00030 | lr [0.000055, 0.000055] | Val@20 0.9939  Test@20 0.9657\n",
            "Epoch 427 | loss 0.00029 | lr [0.000054, 0.000054] | Val@20 0.9940  Test@20 0.9700\n",
            "Epoch 428 | loss 0.00030 | lr [0.000052, 0.000052] | Val@20 0.9940  Test@20 0.9690\n",
            "Epoch 429 | loss 0.00030 | lr [0.000051, 0.000051] | Val@20 0.9940  Test@20 0.9691\n",
            "Epoch 430 | loss 0.00031 | lr [0.000050, 0.000050] | Val@20 0.9946  Test@20 0.9695\n",
            "Best: epoch 430 | Val@20 0.9946 | Test@20 0.9695\n",
            "Epoch 431 | loss 0.00030 | lr [0.000048, 0.000048] | Val@20 0.9941  Test@20 0.9715\n",
            "Best Test: epoch 431 | Val@20 0.9941 | Test@20 0.9715\n",
            "Epoch 432 | loss 0.00031 | lr [0.000047, 0.000047] | Val@20 0.9941  Test@20 0.9701\n",
            "Epoch 433 | loss 0.00028 | lr [0.000045, 0.000045] | Val@20 0.9942  Test@20 0.9693\n",
            "Epoch 434 | loss 0.00029 | lr [0.000044, 0.000044] | Val@20 0.9940  Test@20 0.9704\n",
            "Epoch 435 | loss 0.00028 | lr [0.000043, 0.000043] | Val@20 0.9940  Test@20 0.9712\n",
            "Epoch 436 | loss 0.00030 | lr [0.000042, 0.000042] | Val@20 0.9943  Test@20 0.9702\n",
            "Epoch 437 | loss 0.00029 | lr [0.000040, 0.000040] | Val@20 0.9941  Test@20 0.9711\n",
            "Epoch 438 | loss 0.00029 | lr [0.000039, 0.000039] | Val@20 0.9944  Test@20 0.9683\n",
            "Epoch 439 | loss 0.00028 | lr [0.000038, 0.000038] | Val@20 0.9943  Test@20 0.9687\n",
            "Epoch 440 | loss 0.00027 | lr [0.000037, 0.000037] | Val@20 0.9943  Test@20 0.9707\n",
            "Epoch 441 | loss 0.00028 | lr [0.000035, 0.000035] | Val@20 0.9942  Test@20 0.9691\n",
            "Epoch 442 | loss 0.00028 | lr [0.000034, 0.000034] | Val@20 0.9940  Test@20 0.9685\n",
            "Epoch 443 | loss 0.00030 | lr [0.000033, 0.000033] | Val@20 0.9939  Test@20 0.9698\n",
            "Epoch 444 | loss 0.00028 | lr [0.000032, 0.000032] | Val@20 0.9947  Test@20 0.9700\n",
            "Best: epoch 444 | Val@20 0.9947 | Test@20 0.9700\n",
            "Epoch 445 | loss 0.00029 | lr [0.000031, 0.000031] | Val@20 0.9944  Test@20 0.9688\n",
            "Epoch 446 | loss 0.00029 | lr [0.000030, 0.000030] | Val@20 0.9942  Test@20 0.9710\n",
            "Epoch 447 | loss 0.00029 | lr [0.000029, 0.000029] | Val@20 0.9943  Test@20 0.9700\n",
            "Epoch 448 | loss 0.00028 | lr [0.000028, 0.000028] | Val@20 0.9944  Test@20 0.9698\n",
            "Epoch 449 | loss 0.00028 | lr [0.000026, 0.000026] | Val@20 0.9944  Test@20 0.9697\n",
            "Epoch 450 | loss 0.00027 | lr [0.000025, 0.000025] | Val@20 0.9936  Test@20 0.9718\n",
            "Best Test: epoch 450 | Val@20 0.9936 | Test@20 0.9718\n",
            "Epoch 451 | loss 0.00028 | lr [0.000024, 0.000024] | Val@20 0.9942  Test@20 0.9716\n",
            "Epoch 452 | loss 0.00028 | lr [0.000023, 0.000023] | Val@20 0.9941  Test@20 0.9714\n",
            "Epoch 453 | loss 0.00028 | lr [0.000023, 0.000023] | Val@20 0.9940  Test@20 0.9711\n",
            "Epoch 454 | loss 0.00027 | lr [0.000022, 0.000022] | Val@20 0.9939  Test@20 0.9724\n",
            "Best Test: epoch 454 | Val@20 0.9939 | Test@20 0.9724\n",
            "Epoch 455 | loss 0.00027 | lr [0.000021, 0.000021] | Val@20 0.9944  Test@20 0.9708\n",
            "Epoch 456 | loss 0.00027 | lr [0.000020, 0.000020] | Val@20 0.9943  Test@20 0.9707\n",
            "Epoch 457 | loss 0.00028 | lr [0.000019, 0.000019] | Val@20 0.9944  Test@20 0.9714\n",
            "Epoch 458 | loss 0.00027 | lr [0.000018, 0.000018] | Val@20 0.9945  Test@20 0.9699\n",
            "Epoch 459 | loss 0.00027 | lr [0.000017, 0.000017] | Val@20 0.9942  Test@20 0.9719\n",
            "Epoch 460 | loss 0.00028 | lr [0.000016, 0.000016] | Val@20 0.9943  Test@20 0.9717\n",
            "Epoch 461 | loss 0.00026 | lr [0.000016, 0.000016] | Val@20 0.9939  Test@20 0.9722\n",
            "Epoch 462 | loss 0.00027 | lr [0.000015, 0.000015] | Val@20 0.9941  Test@20 0.9717\n",
            "Epoch 463 | loss 0.00027 | lr [0.000014, 0.000014] | Val@20 0.9939  Test@20 0.9720\n",
            "Epoch 464 | loss 0.00026 | lr [0.000013, 0.000013] | Val@20 0.9939  Test@20 0.9726\n",
            "Best Test: epoch 464 | Val@20 0.9939 | Test@20 0.9726\n",
            "Epoch 465 | loss 0.00027 | lr [0.000013, 0.000013] | Val@20 0.9941  Test@20 0.9727\n",
            "Best Test: epoch 465 | Val@20 0.9941 | Test@20 0.9727\n",
            "Epoch 466 | loss 0.00026 | lr [0.000012, 0.000012] | Val@20 0.9942  Test@20 0.9717\n",
            "Epoch 467 | loss 0.00028 | lr [0.000011, 0.000011] | Val@20 0.9940  Test@20 0.9722\n",
            "Epoch 468 | loss 0.00027 | lr [0.000010, 0.000010] | Val@20 0.9939  Test@20 0.9708\n",
            "Epoch 469 | loss 0.00028 | lr [0.000010, 0.000010] | Val@20 0.9940  Test@20 0.9712\n",
            "Epoch 470 | loss 0.00028 | lr [0.000009, 0.000009] | Val@20 0.9941  Test@20 0.9714\n",
            "Epoch 471 | loss 0.00027 | lr [0.000009, 0.000009] | Val@20 0.9941  Test@20 0.9717\n",
            "Epoch 472 | loss 0.00027 | lr [0.000008, 0.000008] | Val@20 0.9940  Test@20 0.9723\n",
            "Epoch 473 | loss 0.00027 | lr [0.000007, 0.000007] | Val@20 0.9941  Test@20 0.9723\n",
            "Epoch 474 | loss 0.00027 | lr [0.000007, 0.000007] | Val@20 0.9942  Test@20 0.9721\n",
            "Epoch 475 | loss 0.00027 | lr [0.000006, 0.000006] | Val@20 0.9942  Test@20 0.9716\n",
            "Epoch 476 | loss 0.00028 | lr [0.000006, 0.000006] | Val@20 0.9941  Test@20 0.9719\n",
            "Epoch 477 | loss 0.00027 | lr [0.000005, 0.000005] | Val@20 0.9943  Test@20 0.9720\n",
            "Epoch 478 | loss 0.00027 | lr [0.000005, 0.000005] | Val@20 0.9942  Test@20 0.9718\n",
            "Epoch 479 | loss 0.00027 | lr [0.000005, 0.000005] | Val@20 0.9941  Test@20 0.9721\n",
            "Epoch 480 | loss 0.00027 | lr [0.000004, 0.000004] | Val@20 0.9942  Test@20 0.9718\n",
            "Epoch 481 | loss 0.00027 | lr [0.000004, 0.000004] | Val@20 0.9943  Test@20 0.9710\n",
            "Epoch 482 | loss 0.00027 | lr [0.000003, 0.000003] | Val@20 0.9943  Test@20 0.9710\n",
            "Epoch 483 | loss 0.00026 | lr [0.000003, 0.000003] | Val@20 0.9942  Test@20 0.9715\n",
            "Epoch 484 | loss 0.00028 | lr [0.000003, 0.000003] | Val@20 0.9942  Test@20 0.9715\n",
            "Early stopping at epoch 484 (no improvement for 40 epochs).\n",
            "\n",
            "Best: epoch 444 | Val@20 0.9947 | Test@20 0.9700\n",
            "\n",
            "Best Test: epoch 465 | Val@20 0.9941 | Test@20 0.9727\n",
            "Plot saved to /content/drive/MyDrive/CS145/Week11-GCN_SciBert_Ini//results_seed7/4267_Enh4Sage_T26_plot.png\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}